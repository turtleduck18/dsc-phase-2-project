{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This project was an exercise in multiple linear regression, using the Housing dataset from King County. The process follows the OSEM-I Data Science work flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O: Obtain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The first step is to upload the data and load the numpy/pandas libraries, then take a \n",
    "##look at what the dataframe looks like, inspect column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "kchouse = pd.read_csv('data/kc_house_data.csv')\n",
    "kchouse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kchouse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new = kchouse.drop(['date', 'view', 'sqft_above', 'sqft_basement', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'], axis=1)\n",
    "kc_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I created a correlation heatmap to ID any potential independent variables that\n",
    "#exhibit multicolinearity, as that would violate one of the necessary assumptions\n",
    "#we need for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new['waterfront'] = kc_new['waterfront'].fillna(0.0)\n",
    "kc_new.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wanted to make sure there was one entry per ID, and drop any double-entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new['id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next steps, Scrub and Explore, do mean separate things, but tend go hand in \n",
    "#hand. As we Explore more representations of our data, we gain more particular \n",
    "#insight into its characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I noticed there were null values in the waterfront column. The values 1 and 0 \n",
    "#represent if the property is on a waterfront or not. I'm making and educated \n",
    "#assumption that that means there is no waterfront, so I changed the null values \n",
    "#to a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some notes: # Mean price is $540,296.57 \n",
    "             # std $367368.14\n",
    "             # min $78,000.0 \n",
    "             # 25% $322,000.0\n",
    "             # 50% $450,000.0,\n",
    "             # 75% $645000.0\n",
    "             # max $7,700,000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### big ol scatter matrix to help discern between continuous/categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(kc_new, figsize=[12,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = kc_new.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True, annot = True, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kc_new.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates()\n",
    "df[(df.cc>.75) & (df.cc <1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_and_scatter(feature, title, featurename):\n",
    "    figure, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
    "    ax[0].hist(feature)\n",
    "    ax[1].scatter(feature, kc_new['price'])\n",
    "    ax[0].set_xlabel(featurename)\n",
    "    ax[1].set_xlabel(featurename)\n",
    "    ax[1].set_ylabel('price')\n",
    "    figure.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in kc_new.columns:\n",
    "    hist_and_scatter(\n",
    "        kc_new[col], f'Distribution and Relationship of Sale Price and {col}', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variable correlations\n",
    "corrs = kc_new.corr()\n",
    "\n",
    "# Show variables with the highest relationship to SalePrice\n",
    "corrs.abs().sort_values('price', ascending=False)['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The variables with the clearest linear relationship to price are sqft_living,\n",
    "# grade, and # bathrooms. Bedrooms/Presence of a Waterfront, and Floors\n",
    "# have a weak positive correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS with most correlated feature ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kc_new['sqft_living'] \n",
    "Y = kc_new['price']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(Y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = kc_new.drop(['price'], axis = 1)\n",
    "y = kc_new['price']\n",
    "X = sm.add_constant(X) \n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Are we meeting our assumptions for MLS? #\n",
    "## Multicolinearity, Homoscedascity, Normality ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####: drop id & yrbuilt\n",
    "### .608 - .611 (ols)\n",
    "### trying to drop bathrooms as well to address potential mc\n",
    "### .608, basically no difference\n",
    "### drop just id and use yr_built as numeric\n",
    "### .655, qq plot isn't greatest\n",
    "##### id and yr_built are very gone\n",
    "### drop id, yr_built, sqft_lot, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_lot']\n",
    "categoricals = ['waterfront', 'bedrooms','grade', 'floors', 'bathrooms', 'condition']\n",
    "kccat = kc_new[categoricals]\n",
    "kccon = kc_new[continuous]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_ohe = pd.get_dummies(kccat['condition'], prefix = 'condition', drop_first=True)\n",
    "waterfront_ohe =pd.get_dummies(kccat['waterfront'], prefix = 'waterfront', drop_first=True)\n",
    "grade_ohe= pd.get_dummies(kccat['grade'], prefix = 'grade', drop_first=True)\n",
    "bed_ohe= pd.get_dummies(kccat['bedrooms'], prefix = 'bed', drop_first=True)\n",
    "bath_ohe= pd.get_dummies(kccat['bathrooms'], prefix = 'bath', drop_first=True)\n",
    "floors_ohe= pd.get_dummies(kccat['floors'], prefix = 'floors', drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_concat = pd.concat([waterfront_ohe, bed_ohe, floors_ohe, grade_ohe, bath_ohe, condition_ohe], axis = 1)\n",
    "ohe_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_log = np.log(kc_new['price'])\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "price_log_norm = normalize(price_log)\n",
    "price_log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log and normalize features\n",
    "log_names = [f'{column}_log' for column in kccon.columns]\n",
    "\n",
    "kccon_log = np.log(kccon)\n",
    "kccon_log.columns = log_names\n",
    "\n",
    "log_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "kc_log_norm = kccon_log.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = pd.concat([ohe_concat, kc_log_norm, price_log_norm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed.columns = preprocessed.columns.str.replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed.drop(['price'], axis = 1)\n",
    "y = preprocessed['price']\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Train Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed.drop(['price'], axis = 1)\n",
    "y = preprocessed['price']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "preds = linreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_train.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates()\n",
    "df[(df.cc>.75) & (df.cc <1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" \n",
    "    Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype='float64')\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = stepwise_selection(X_train, y_train, verbose = True)\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fin = X_train[result]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model = sm.OLS(y_train,X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Feature Selection with Scikit #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "linreg = LinearRegression()\n",
    "selector = RFE(linreg, n_features_to_select = 25)\n",
    "selector = selector.fit(X_fin, y_train.values.ravel()) # convert y to31d np array to prevent DataConversionWarning\n",
    "selector.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = X_fin.columns[selector.support_ ]\n",
    "linreg.fit(X_fin[selected_columns],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = linreg.predict(X_fin[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_Residual = np.sum((y-yhat)**2)\n",
    "SS_Total = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X_fin[selected_columns].shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y - preds\n",
    "\n",
    "plt.scatter(preds, residuals)\n",
    "plt.hlines(0, preds.min(), preds.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
