{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: \n",
    "* Student pace: self paced / part time / full time\n",
    "* Scheduled project review date/time: \n",
    "* Instructor name: \n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was an exercise in multiple linear regression, using the Housing dataset from King County. The process follows the OSEM-I Data Science work flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to upload the data and load the numpy/pandas libraries, then take a look at what the dataframe looks like, inspect column names, and drop unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>10/13/2014</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2/25/2015</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2/18/2015</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  10/13/2014  221900.0         3       1.00         1180   \n",
       "1  6414100192   12/9/2014  538000.0         3       2.25         2570   \n",
       "2  5631500400   2/25/2015  180000.0         2       1.00          770   \n",
       "3  2487200875   12/9/2014  604000.0         4       3.00         1960   \n",
       "4  1954400510   2/18/2015  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  ...  grade  sqft_above  sqft_basement  \\\n",
       "0      5650     1.0         NaN   0.0  ...      7        1180            0.0   \n",
       "1      7242     2.0         0.0   0.0  ...      7        2170          400.0   \n",
       "2     10000     1.0         0.0   0.0  ...      6         770            0.0   \n",
       "3      5000     1.0         0.0   0.0  ...      7        1050          910.0   \n",
       "4      8080     1.0         0.0   0.0  ...      8        1680            0.0   \n",
       "\n",
       "  yr_built  yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0     1955           0.0    98178  47.5112 -122.257           1340        5650  \n",
       "1     1951        1991.0    98125  47.7210 -122.319           1690        7639  \n",
       "2     1933           NaN    98028  47.7379 -122.233           2720        8062  \n",
       "3     1965           0.0    98136  47.5208 -122.393           1360        5000  \n",
       "4     1987           0.0    98074  47.6168 -122.045           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "kchouse = pd.read_csv('data/kc_house_data.csv')\n",
    "kchouse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kchouse.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   id           21597 non-null  int64  \n",
      " 1   price        21597 non-null  float64\n",
      " 2   bedrooms     21597 non-null  int64  \n",
      " 3   bathrooms    21597 non-null  float64\n",
      " 4   sqft_living  21597 non-null  int64  \n",
      " 5   sqft_lot     21597 non-null  int64  \n",
      " 6   floors       21597 non-null  float64\n",
      " 7   waterfront   19221 non-null  float64\n",
      " 8   condition    21597 non-null  int64  \n",
      " 9   grade        21597 non-null  int64  \n",
      " 10  yr_built     21597 non-null  int64  \n",
      "dtypes: float64(4), int64(7)\n",
      "memory usage: 1.8 MB\n"
     ]
    }
   ],
   "source": [
    "kc_new = kchouse.drop(['date', 'view', 'sqft_above', 'sqft_basement', 'yr_renovated', 'zipcode', 'lat', 'long', 'sqft_living15', 'sqft_lot15'], axis=1)\n",
    "kc_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps, Scrub and Explore, do mean separate things, but tend go hand in hand. As we explore more representations of our data, we gain more particular insight into its characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed there were null values in the waterfront column. The values 1 and 0 represent if the property is on a waterfront or not. I'mmaking and educated assumption that that means there is no waterfront, so I changed the null values to a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id     price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  condition  grade  yr_built\n",
       "False  False  False     False      False        False     False   False       False      False  False       21597\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_new['waterfront'] = kc_new['waterfront'].fillna(0.0)\n",
    "kc_new.isna().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to make sure there was one entry per ID, and drop any double-entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new['id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.drop_duplicates(subset = 'id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some notes: # Mean price is $540,296.57 \n",
    "             # std $367368.14\n",
    "             # min $78,000.0 \n",
    "             # 25% $322,000.0\n",
    "             # 50% $450,000.0,\n",
    "             # 75% $645000.0\n",
    "             # max $7,700,000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we continue to explore our data, it's important to keep in mind whether the essential assumptions for Linear Regression are met. We require Linearity between X and Y, lack of Multicolinearity, Homoscedascity, and Normality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter matrix is helpful to get a glance of how the independent variables may interact with each other and address multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(kc_new, figsize=[14,14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a correlation heat map to visualize the Pearson Correlation coefficients between both independent and dependent variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "corr = kc_new.corr()\n",
    "mask = np.zeros_like(corr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "\n",
    "    f, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "    ax = sns.heatmap(corr, mask=mask, square=True, annot = True, cmap = 'coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for colinearity between independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = kc_new.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "# zip the variable name columns (Which were only named level_0 and level_1 by default) in a new column named \"pairs\"\n",
    "df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "# set index to pairs\n",
    "df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "#d rop level columns\n",
    "df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "# rename correlation column as cc rather than 0\n",
    "df.columns = ['cc']\n",
    "\n",
    "# drop duplicates. This could be dangerous if you have variables perfectly correlated with variables other than themselves.\n",
    "# for the sake of exercise, kept it in.\n",
    "df.drop_duplicates()\n",
    "df[(df.cc>.75) & (df.cc <1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_and_scatter(feature, title, featurename):\n",
    "    figure, ax = plt.subplots(1, 2, figsize=(8, 6))\n",
    "    ax[0].hist(feature, bins = 20)\n",
    "    ax[1].scatter(feature, kc_new['price'])\n",
    "    ax[0].set_xlabel(featurename)\n",
    "    ax[1].set_xlabel(featurename)\n",
    "    ax[1].set_ylabel('price')\n",
    "    figure.suptitle(title)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.9])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in kc_new.columns:\n",
    "    hist_and_scatter(\n",
    "        kc_new[col], f'Distribution and Relationship between Price and {col}', col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = kc_new.corr()\n",
    "\n",
    "corrs.abs().sort_values('price', ascending=False)['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the scatter plots, the features with the clearest linear relationship to price are sqft_living, grade, and bathrooms. condition, sqft_lot, yr_built and id have no visible linear relationship with price. The features with the highest correlation coefficient to price are sqft_living (0.701875), grade (0.668020), and bathrooms (0.526229), and the lowest are sqft_lot (0.089111), yr_built  (0.052906), condition (0.034779) and id (0.018525). \n",
    "\n",
    "It's also important to note that sqft_living has a high correlation with grade and bathrooms.\n",
    "(sqft_living, grade) \t    0.762477\n",
    "(sqft_living, bathrooms) \t0.755522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kc_new['bathrooms'],kc_new['sqft_living'])\n",
    "plt.xlabel('bathrooms')\n",
    "plt.ylabel('sqft_living')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(kc_new['grade'],kc_new['sqft_living'])\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('sqft_living')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS with most correlated feature ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.493</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.493</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>2.080e+04</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:17:51</td>     <th>  Log-Likelihood:    </th> <td>-2.9764e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 21420</td>      <th>  AIC:               </th>  <td>5.953e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 21418</td>      <th>  BIC:               </th>  <td>5.953e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td>-4.475e+04</td> <td> 4437.427</td> <td>  -10.084</td> <td> 0.000</td> <td>-5.34e+04</td> <td>-3.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th> <td>  281.0616</td> <td>    1.949</td> <td>  144.207</td> <td> 0.000</td> <td>  277.241</td> <td>  284.882</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14687.843</td> <th>  Durbin-Watson:     </th>  <td>   1.989</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>538458.356</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.822</td>   <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>26.905</td>   <th>  Cond. No.          </th>  <td>5.64e+03</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 5.64e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.493\n",
       "Model:                            OLS   Adj. R-squared:                  0.493\n",
       "Method:                 Least Squares   F-statistic:                 2.080e+04\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:17:51   Log-Likelihood:            -2.9764e+05\n",
       "No. Observations:               21420   AIC:                         5.953e+05\n",
       "Df Residuals:                   21418   BIC:                         5.953e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const       -4.475e+04   4437.427    -10.084      0.000   -5.34e+04   -3.61e+04\n",
       "sqft_living   281.0616      1.949    144.207      0.000     277.241     284.882\n",
       "==============================================================================\n",
       "Omnibus:                    14687.843   Durbin-Watson:                   1.989\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           538458.356\n",
       "Skew:                           2.822   Prob(JB):                         0.00\n",
       "Kurtosis:                      26.905   Cond. No.                     5.64e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 5.64e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = kc_new['sqft_living'] \n",
    "y = kc_new['price']\n",
    "\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th>  <td>   0.646</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.646</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   3908.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:17:57</td>     <th>  Log-Likelihood:    </th> <td>-2.9378e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 21420</td>      <th>  AIC:               </th>  <td>5.876e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 21409</td>      <th>  BIC:               </th>  <td>5.877e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>       <td> 6.688e+06</td> <td>  1.3e+05</td> <td>   51.280</td> <td> 0.000</td> <td> 6.43e+06</td> <td> 6.94e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>id</th>          <td>-1.552e-06</td> <td> 5.25e-07</td> <td>   -2.955</td> <td> 0.003</td> <td>-2.58e-06</td> <td>-5.23e-07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bedrooms</th>    <td>-4.296e+04</td> <td> 2064.444</td> <td>  -20.808</td> <td> 0.000</td> <td> -4.7e+04</td> <td>-3.89e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bathrooms</th>   <td> 5.152e+04</td> <td> 3472.880</td> <td>   14.835</td> <td> 0.000</td> <td> 4.47e+04</td> <td> 5.83e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living</th> <td>  176.9585</td> <td>    3.315</td> <td>   53.380</td> <td> 0.000</td> <td>  170.461</td> <td>  183.456</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>    <td>   -0.2569</td> <td>    0.037</td> <td>   -6.909</td> <td> 0.000</td> <td>   -0.330</td> <td>   -0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors</th>      <td>  2.11e+04</td> <td> 3474.091</td> <td>    6.074</td> <td> 0.000</td> <td> 1.43e+04</td> <td> 2.79e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront</th>  <td>  7.55e+05</td> <td> 1.84e+04</td> <td>   41.078</td> <td> 0.000</td> <td> 7.19e+05</td> <td> 7.91e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition</th>   <td> 1.933e+04</td> <td> 2507.177</td> <td>    7.710</td> <td> 0.000</td> <td> 1.44e+04</td> <td> 2.42e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade</th>       <td> 1.304e+05</td> <td> 2167.181</td> <td>   60.167</td> <td> 0.000</td> <td> 1.26e+05</td> <td> 1.35e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>yr_built</th>    <td>-3840.9998</td> <td>   67.041</td> <td>  -57.293</td> <td> 0.000</td> <td>-3972.406</td> <td>-3709.594</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>15771.211</td> <th>  Durbin-Watson:     </th>  <td>   1.983</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>1013819.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 2.948</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>36.184</td>   <th>  Cond. No.          </th>  <td>4.72e+11</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.72e+11. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.646\n",
       "Model:                            OLS   Adj. R-squared:                  0.646\n",
       "Method:                 Least Squares   F-statistic:                     3908.\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:17:57   Log-Likelihood:            -2.9378e+05\n",
       "No. Observations:               21420   AIC:                         5.876e+05\n",
       "Df Residuals:                   21409   BIC:                         5.877e+05\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "const        6.688e+06    1.3e+05     51.280      0.000    6.43e+06    6.94e+06\n",
       "id          -1.552e-06   5.25e-07     -2.955      0.003   -2.58e-06   -5.23e-07\n",
       "bedrooms    -4.296e+04   2064.444    -20.808      0.000    -4.7e+04   -3.89e+04\n",
       "bathrooms    5.152e+04   3472.880     14.835      0.000    4.47e+04    5.83e+04\n",
       "sqft_living   176.9585      3.315     53.380      0.000     170.461     183.456\n",
       "sqft_lot       -0.2569      0.037     -6.909      0.000      -0.330      -0.184\n",
       "floors        2.11e+04   3474.091      6.074      0.000    1.43e+04    2.79e+04\n",
       "waterfront    7.55e+05   1.84e+04     41.078      0.000    7.19e+05    7.91e+05\n",
       "condition    1.933e+04   2507.177      7.710      0.000    1.44e+04    2.42e+04\n",
       "grade        1.304e+05   2167.181     60.167      0.000    1.26e+05    1.35e+05\n",
       "yr_built    -3840.9998     67.041    -57.293      0.000   -3972.406   -3709.594\n",
       "==============================================================================\n",
       "Omnibus:                    15771.211   Durbin-Watson:                   1.983\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1013819.083\n",
       "Skew:                           2.948   Prob(JB):                         0.00\n",
       "Kurtosis:                      36.184   Cond. No.                     4.72e+11\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.72e+11. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = kc_new.drop(['price'], axis = 1)\n",
    "y = kc_new['price']\n",
    "X = sm.add_constant(X) \n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####: drop id & yrbuilt\n",
    "### .608 - .611 (ols)\n",
    "### trying to drop bathrooms as well to address potential mc\n",
    "### .608, basically no difference\n",
    "### drop just id and use yr_built as numeric\n",
    "### .655, qq plot isn't greatest\n",
    "##### id and yr_built are very gone\n",
    "### drop id, yr_built, sqft_lot, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_lot']\n",
    "categoricals = ['waterfront', 'bedrooms','grade', 'floors', 'bathrooms', 'condition']\n",
    "kccat = kc_new[categoricals]\n",
    "kccon = kc_new[continuous]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_ohe = pd.get_dummies(kccat['condition'], prefix = 'condition', drop_first=True)\n",
    "waterfront_ohe =pd.get_dummies(kccat['waterfront'], prefix = 'waterfront', drop_first=True)\n",
    "grade_ohe= pd.get_dummies(kccat['grade'], prefix = 'grade', drop_first=True)\n",
    "bed_ohe= pd.get_dummies(kccat['bedrooms'], prefix = 'bed', drop_first=True)\n",
    "bath_ohe= pd.get_dummies(kccat['bathrooms'], prefix = 'bath', drop_first=True)\n",
    "floors_ohe= pd.get_dummies(kccat['floors'], prefix = 'floors', drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waterfront_1.0</th>\n",
       "      <th>bed_2</th>\n",
       "      <th>bed_3</th>\n",
       "      <th>bed_4</th>\n",
       "      <th>bed_5</th>\n",
       "      <th>bed_6</th>\n",
       "      <th>bed_7</th>\n",
       "      <th>bed_8</th>\n",
       "      <th>bed_9</th>\n",
       "      <th>bed_10</th>\n",
       "      <th>...</th>\n",
       "      <th>bath_6.25</th>\n",
       "      <th>bath_6.5</th>\n",
       "      <th>bath_6.75</th>\n",
       "      <th>bath_7.5</th>\n",
       "      <th>bath_7.75</th>\n",
       "      <th>bath_8.0</th>\n",
       "      <th>condition_2</th>\n",
       "      <th>condition_3</th>\n",
       "      <th>condition_4</th>\n",
       "      <th>condition_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21420 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       waterfront_1.0  bed_2  bed_3  bed_4  bed_5  bed_6  bed_7  bed_8  bed_9  \\\n",
       "0                   0      0      1      0      0      0      0      0      0   \n",
       "1                   0      0      1      0      0      0      0      0      0   \n",
       "2                   0      1      0      0      0      0      0      0      0   \n",
       "3                   0      0      0      1      0      0      0      0      0   \n",
       "4                   0      0      1      0      0      0      0      0      0   \n",
       "...               ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "21592               0      0      1      0      0      0      0      0      0   \n",
       "21593               0      0      0      1      0      0      0      0      0   \n",
       "21594               0      1      0      0      0      0      0      0      0   \n",
       "21595               0      0      1      0      0      0      0      0      0   \n",
       "21596               0      1      0      0      0      0      0      0      0   \n",
       "\n",
       "       bed_10  ...  bath_6.25  bath_6.5  bath_6.75  bath_7.5  bath_7.75  \\\n",
       "0           0  ...          0         0          0         0          0   \n",
       "1           0  ...          0         0          0         0          0   \n",
       "2           0  ...          0         0          0         0          0   \n",
       "3           0  ...          0         0          0         0          0   \n",
       "4           0  ...          0         0          0         0          0   \n",
       "...       ...  ...        ...       ...        ...       ...        ...   \n",
       "21592       0  ...          0         0          0         0          0   \n",
       "21593       0  ...          0         0          0         0          0   \n",
       "21594       0  ...          0         0          0         0          0   \n",
       "21595       0  ...          0         0          0         0          0   \n",
       "21596       0  ...          0         0          0         0          0   \n",
       "\n",
       "       bath_8.0  condition_2  condition_3  condition_4  condition_5  \n",
       "0             0            0            1            0            0  \n",
       "1             0            0            1            0            0  \n",
       "2             0            0            1            0            0  \n",
       "3             0            0            0            0            1  \n",
       "4             0            0            1            0            0  \n",
       "...         ...          ...          ...          ...          ...  \n",
       "21592         0            0            1            0            0  \n",
       "21593         0            0            1            0            0  \n",
       "21594         0            0            1            0            0  \n",
       "21595         0            0            1            0            0  \n",
       "21596         0            0            1            0            0  \n",
       "\n",
       "[21420 rows x 59 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_concat = pd.concat([waterfront_ohe, bed_ohe, grade_ohe, floors_ohe, bath_ohe, condition_ohe], axis = 1)\n",
    "ohe_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1.402879\n",
       "1        0.278512\n",
       "2       -1.800182\n",
       "3        0.498200\n",
       "4        0.177040\n",
       "           ...   \n",
       "21592   -0.484228\n",
       "21593   -0.284199\n",
       "21594   -0.274253\n",
       "21595   -0.284199\n",
       "21596   -0.678406\n",
       "Name: price, Length: 21420, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_log = np.log(kc_new['price'])\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "price_log_norm = normalize(price_log)\n",
    "price_log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sqft_living_log', 'sqft_lot_log']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log and normalize features\n",
    "log_names = [f'{column}_log' for column in kccon.columns]\n",
    "\n",
    "kccon_log = np.log(kccon)\n",
    "kccon_log.columns = log_names\n",
    "\n",
    "log_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "kc_log_norm = kccon_log.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = pd.concat([ohe_concat, kc_log_norm, price_log_norm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed.columns = preprocessed.columns.str.replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21420 entries, 0 to 21596\n",
      "Data columns (total 62 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   waterfront_1_0   21420 non-null  uint8  \n",
      " 1   bed_2            21420 non-null  uint8  \n",
      " 2   bed_3            21420 non-null  uint8  \n",
      " 3   bed_4            21420 non-null  uint8  \n",
      " 4   bed_5            21420 non-null  uint8  \n",
      " 5   bed_6            21420 non-null  uint8  \n",
      " 6   bed_7            21420 non-null  uint8  \n",
      " 7   bed_8            21420 non-null  uint8  \n",
      " 8   bed_9            21420 non-null  uint8  \n",
      " 9   bed_10           21420 non-null  uint8  \n",
      " 10  bed_11           21420 non-null  uint8  \n",
      " 11  bed_33           21420 non-null  uint8  \n",
      " 12  grade_4          21420 non-null  uint8  \n",
      " 13  grade_5          21420 non-null  uint8  \n",
      " 14  grade_6          21420 non-null  uint8  \n",
      " 15  grade_7          21420 non-null  uint8  \n",
      " 16  grade_8          21420 non-null  uint8  \n",
      " 17  grade_9          21420 non-null  uint8  \n",
      " 18  grade_10         21420 non-null  uint8  \n",
      " 19  grade_11         21420 non-null  uint8  \n",
      " 20  grade_12         21420 non-null  uint8  \n",
      " 21  grade_13         21420 non-null  uint8  \n",
      " 22  floors_1_5       21420 non-null  uint8  \n",
      " 23  floors_2_0       21420 non-null  uint8  \n",
      " 24  floors_2_5       21420 non-null  uint8  \n",
      " 25  floors_3_0       21420 non-null  uint8  \n",
      " 26  floors_3_5       21420 non-null  uint8  \n",
      " 27  bath_0_75        21420 non-null  uint8  \n",
      " 28  bath_1_0         21420 non-null  uint8  \n",
      " 29  bath_1_25        21420 non-null  uint8  \n",
      " 30  bath_1_5         21420 non-null  uint8  \n",
      " 31  bath_1_75        21420 non-null  uint8  \n",
      " 32  bath_2_0         21420 non-null  uint8  \n",
      " 33  bath_2_25        21420 non-null  uint8  \n",
      " 34  bath_2_5         21420 non-null  uint8  \n",
      " 35  bath_2_75        21420 non-null  uint8  \n",
      " 36  bath_3_0         21420 non-null  uint8  \n",
      " 37  bath_3_25        21420 non-null  uint8  \n",
      " 38  bath_3_5         21420 non-null  uint8  \n",
      " 39  bath_3_75        21420 non-null  uint8  \n",
      " 40  bath_4_0         21420 non-null  uint8  \n",
      " 41  bath_4_25        21420 non-null  uint8  \n",
      " 42  bath_4_5         21420 non-null  uint8  \n",
      " 43  bath_4_75        21420 non-null  uint8  \n",
      " 44  bath_5_0         21420 non-null  uint8  \n",
      " 45  bath_5_25        21420 non-null  uint8  \n",
      " 46  bath_5_5         21420 non-null  uint8  \n",
      " 47  bath_5_75        21420 non-null  uint8  \n",
      " 48  bath_6_0         21420 non-null  uint8  \n",
      " 49  bath_6_25        21420 non-null  uint8  \n",
      " 50  bath_6_5         21420 non-null  uint8  \n",
      " 51  bath_6_75        21420 non-null  uint8  \n",
      " 52  bath_7_5         21420 non-null  uint8  \n",
      " 53  bath_7_75        21420 non-null  uint8  \n",
      " 54  bath_8_0         21420 non-null  uint8  \n",
      " 55  condition_2      21420 non-null  uint8  \n",
      " 56  condition_3      21420 non-null  uint8  \n",
      " 57  condition_4      21420 non-null  uint8  \n",
      " 58  condition_5      21420 non-null  uint8  \n",
      " 59  sqft_living_log  21420 non-null  float64\n",
      " 60  sqft_lot_log     21420 non-null  float64\n",
      " 61  price            21420 non-null  float64\n",
      "dtypes: float64(3), uint8(59)\n",
      "memory usage: 1.9 MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.612</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.611</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   552.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:18:31</td>     <th>  Log-Likelihood:    </th> <td> -20248.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 21420</td>      <th>  AIC:               </th> <td>4.062e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 21358</td>      <th>  BIC:               </th> <td>4.111e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    61</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -0.7743</td> <td>    0.712</td> <td>   -1.087</td> <td> 0.277</td> <td>   -2.171</td> <td>    0.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront_1_0</th>  <td>    1.1625</td> <td>    0.053</td> <td>   22.064</td> <td> 0.000</td> <td>    1.059</td> <td>    1.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>   -0.1297</td> <td>    0.049</td> <td>   -2.666</td> <td> 0.008</td> <td>   -0.225</td> <td>   -0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_3</th>           <td>   -0.4080</td> <td>    0.049</td> <td>   -8.341</td> <td> 0.000</td> <td>   -0.504</td> <td>   -0.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_4</th>           <td>   -0.4491</td> <td>    0.050</td> <td>   -8.949</td> <td> 0.000</td> <td>   -0.547</td> <td>   -0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_5</th>           <td>   -0.4534</td> <td>    0.053</td> <td>   -8.584</td> <td> 0.000</td> <td>   -0.557</td> <td>   -0.350</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_6</th>           <td>   -0.4830</td> <td>    0.064</td> <td>   -7.516</td> <td> 0.000</td> <td>   -0.609</td> <td>   -0.357</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_7</th>           <td>   -0.5505</td> <td>    0.116</td> <td>   -4.746</td> <td> 0.000</td> <td>   -0.778</td> <td>   -0.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_8</th>           <td>   -0.2266</td> <td>    0.183</td> <td>   -1.236</td> <td> 0.216</td> <td>   -0.586</td> <td>    0.133</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_9</th>           <td>    0.0141</td> <td>    0.286</td> <td>    0.049</td> <td> 0.961</td> <td>   -0.546</td> <td>    0.574</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_10</th>          <td>   -0.1923</td> <td>    0.369</td> <td>   -0.521</td> <td> 0.602</td> <td>   -0.915</td> <td>    0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_11</th>          <td>   -0.3044</td> <td>    0.626</td> <td>   -0.486</td> <td> 0.627</td> <td>   -1.532</td> <td>    0.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_33</th>          <td>    0.3149</td> <td>    0.626</td> <td>    0.503</td> <td> 0.615</td> <td>   -0.912</td> <td>    1.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_4</th>         <td>   -0.4165</td> <td>    0.637</td> <td>   -0.654</td> <td> 0.513</td> <td>   -1.665</td> <td>    0.832</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.4218</td> <td>    0.630</td> <td>   -0.669</td> <td> 0.503</td> <td>   -1.657</td> <td>    0.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>         <td>   -0.1394</td> <td>    0.630</td> <td>   -0.221</td> <td> 0.825</td> <td>   -1.374</td> <td>    1.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>    0.2279</td> <td>    0.630</td> <td>    0.362</td> <td> 0.717</td> <td>   -1.007</td> <td>    1.463</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>         <td>    0.6281</td> <td>    0.630</td> <td>    0.997</td> <td> 0.319</td> <td>   -0.607</td> <td>    1.863</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    1.0934</td> <td>    0.630</td> <td>    1.735</td> <td> 0.083</td> <td>   -0.142</td> <td>    2.329</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    1.4505</td> <td>    0.631</td> <td>    2.300</td> <td> 0.021</td> <td>    0.214</td> <td>    2.687</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    1.7677</td> <td>    0.631</td> <td>    2.800</td> <td> 0.005</td> <td>    0.530</td> <td>    3.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_12</th>        <td>    2.1855</td> <td>    0.635</td> <td>    3.444</td> <td> 0.001</td> <td>    0.942</td> <td>    3.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_13</th>        <td>    2.6722</td> <td>    0.661</td> <td>    4.045</td> <td> 0.000</td> <td>    1.377</td> <td>    3.967</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.2894</td> <td>    0.016</td> <td>   18.085</td> <td> 0.000</td> <td>    0.258</td> <td>    0.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.0999</td> <td>    0.013</td> <td>   -7.807</td> <td> 0.000</td> <td>   -0.125</td> <td>   -0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_5</th>      <td>    0.1745</td> <td>    0.051</td> <td>    3.422</td> <td> 0.001</td> <td>    0.075</td> <td>    0.274</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_0</th>      <td>    0.0560</td> <td>    0.029</td> <td>    1.900</td> <td> 0.057</td> <td>   -0.002</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_5</th>      <td>    0.0954</td> <td>    0.238</td> <td>    0.401</td> <td> 0.688</td> <td>   -0.371</td> <td>    0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_0_75</th>       <td>    0.6567</td> <td>    0.322</td> <td>    2.041</td> <td> 0.041</td> <td>    0.026</td> <td>    1.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.5819</td> <td>    0.312</td> <td>    1.863</td> <td> 0.062</td> <td>   -0.030</td> <td>    1.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_25</th>       <td>    0.4886</td> <td>    0.375</td> <td>    1.302</td> <td> 0.193</td> <td>   -0.247</td> <td>    1.224</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_5</th>        <td>    0.4705</td> <td>    0.313</td> <td>    1.504</td> <td> 0.133</td> <td>   -0.143</td> <td>    1.083</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_75</th>       <td>    0.5325</td> <td>    0.313</td> <td>    1.704</td> <td> 0.088</td> <td>   -0.080</td> <td>    1.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_0</th>        <td>    0.5153</td> <td>    0.313</td> <td>    1.648</td> <td> 0.099</td> <td>   -0.098</td> <td>    1.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_25</th>       <td>    0.4898</td> <td>    0.313</td> <td>    1.566</td> <td> 0.117</td> <td>   -0.123</td> <td>    1.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>    0.3839</td> <td>    0.313</td> <td>    1.228</td> <td> 0.219</td> <td>   -0.229</td> <td>    0.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_75</th>       <td>    0.5118</td> <td>    0.313</td> <td>    1.635</td> <td> 0.102</td> <td>   -0.102</td> <td>    1.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_0</th>        <td>    0.5540</td> <td>    0.313</td> <td>    1.767</td> <td> 0.077</td> <td>   -0.060</td> <td>    1.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_25</th>       <td>    0.6621</td> <td>    0.314</td> <td>    2.110</td> <td> 0.035</td> <td>    0.047</td> <td>    1.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_5</th>        <td>    0.6085</td> <td>    0.314</td> <td>    1.940</td> <td> 0.052</td> <td>   -0.006</td> <td>    1.223</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_75</th>       <td>    0.8255</td> <td>    0.317</td> <td>    2.605</td> <td> 0.009</td> <td>    0.204</td> <td>    1.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_0</th>        <td>    0.7227</td> <td>    0.318</td> <td>    2.275</td> <td> 0.023</td> <td>    0.100</td> <td>    1.345</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_25</th>       <td>    0.8537</td> <td>    0.321</td> <td>    2.659</td> <td> 0.008</td> <td>    0.224</td> <td>    1.483</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_5</th>        <td>    0.6755</td> <td>    0.320</td> <td>    2.114</td> <td> 0.035</td> <td>    0.049</td> <td>    1.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_75</th>       <td>    0.9520</td> <td>    0.340</td> <td>    2.801</td> <td> 0.005</td> <td>    0.286</td> <td>    1.618</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_0</th>        <td>    0.7683</td> <td>    0.342</td> <td>    2.246</td> <td> 0.025</td> <td>    0.098</td> <td>    1.439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_25</th>       <td>    0.9032</td> <td>    0.359</td> <td>    2.512</td> <td> 0.012</td> <td>    0.199</td> <td>    1.608</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_5</th>        <td>    0.9301</td> <td>    0.373</td> <td>    2.492</td> <td> 0.013</td> <td>    0.198</td> <td>    1.662</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_75</th>       <td>    0.8507</td> <td>    0.447</td> <td>    1.901</td> <td> 0.057</td> <td>   -0.026</td> <td>    1.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_0</th>        <td>    1.1272</td> <td>    0.409</td> <td>    2.754</td> <td> 0.006</td> <td>    0.325</td> <td>    1.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_25</th>       <td>    1.2227</td> <td>    0.550</td> <td>    2.221</td> <td> 0.026</td> <td>    0.144</td> <td>    2.302</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_5</th>        <td>    0.4873</td> <td>    0.543</td> <td>    0.898</td> <td> 0.369</td> <td>   -0.577</td> <td>    1.551</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_75</th>       <td>   -0.0735</td> <td>    0.546</td> <td>   -0.135</td> <td> 0.893</td> <td>   -1.143</td> <td>    0.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_7_5</th>        <td>   -0.3275</td> <td>    0.752</td> <td>   -0.435</td> <td> 0.663</td> <td>   -1.802</td> <td>    1.147</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_7_75</th>       <td>    2.1259</td> <td>    0.726</td> <td>    2.928</td> <td> 0.003</td> <td>    0.703</td> <td>    3.549</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_8_0</th>        <td>    1.0100</td> <td>    0.556</td> <td>    1.817</td> <td> 0.069</td> <td>   -0.079</td> <td>    2.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_2</th>     <td>   -0.1099</td> <td>    0.128</td> <td>   -0.859</td> <td> 0.390</td> <td>   -0.361</td> <td>    0.141</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>     <td>    0.0825</td> <td>    0.119</td> <td>    0.694</td> <td> 0.488</td> <td>   -0.151</td> <td>    0.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.2315</td> <td>    0.119</td> <td>    1.946</td> <td> 0.052</td> <td>   -0.002</td> <td>    0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.4481</td> <td>    0.120</td> <td>    3.745</td> <td> 0.000</td> <td>    0.214</td> <td>    0.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4470</td> <td>    0.009</td> <td>   47.208</td> <td> 0.000</td> <td>    0.428</td> <td>    0.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.0928</td> <td>    0.005</td> <td>  -17.994</td> <td> 0.000</td> <td>   -0.103</td> <td>   -0.083</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 8.233</td> <th>  Durbin-Watson:     </th> <td>   1.983</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.016</td> <th>  Jarque-Bera (JB):  </th> <td>   8.827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.012</td> <th>  Prob(JB):          </th> <td>  0.0121</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.096</td> <th>  Cond. No.          </th> <td>    773.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.612\n",
       "Model:                            OLS   Adj. R-squared:                  0.611\n",
       "Method:                 Least Squares   F-statistic:                     552.7\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:18:31   Log-Likelihood:                -20248.\n",
       "No. Observations:               21420   AIC:                         4.062e+04\n",
       "Df Residuals:                   21358   BIC:                         4.111e+04\n",
       "Df Model:                          61                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -0.7743      0.712     -1.087      0.277      -2.171       0.622\n",
       "waterfront_1_0      1.1625      0.053     22.064      0.000       1.059       1.266\n",
       "bed_2              -0.1297      0.049     -2.666      0.008      -0.225      -0.034\n",
       "bed_3              -0.4080      0.049     -8.341      0.000      -0.504      -0.312\n",
       "bed_4              -0.4491      0.050     -8.949      0.000      -0.547      -0.351\n",
       "bed_5              -0.4534      0.053     -8.584      0.000      -0.557      -0.350\n",
       "bed_6              -0.4830      0.064     -7.516      0.000      -0.609      -0.357\n",
       "bed_7              -0.5505      0.116     -4.746      0.000      -0.778      -0.323\n",
       "bed_8              -0.2266      0.183     -1.236      0.216      -0.586       0.133\n",
       "bed_9               0.0141      0.286      0.049      0.961      -0.546       0.574\n",
       "bed_10             -0.1923      0.369     -0.521      0.602      -0.915       0.531\n",
       "bed_11             -0.3044      0.626     -0.486      0.627      -1.532       0.923\n",
       "bed_33              0.3149      0.626      0.503      0.615      -0.912       1.542\n",
       "grade_4            -0.4165      0.637     -0.654      0.513      -1.665       0.832\n",
       "grade_5            -0.4218      0.630     -0.669      0.503      -1.657       0.814\n",
       "grade_6            -0.1394      0.630     -0.221      0.825      -1.374       1.095\n",
       "grade_7             0.2279      0.630      0.362      0.717      -1.007       1.463\n",
       "grade_8             0.6281      0.630      0.997      0.319      -0.607       1.863\n",
       "grade_9             1.0934      0.630      1.735      0.083      -0.142       2.329\n",
       "grade_10            1.4505      0.631      2.300      0.021       0.214       2.687\n",
       "grade_11            1.7677      0.631      2.800      0.005       0.530       3.005\n",
       "grade_12            2.1855      0.635      3.444      0.001       0.942       3.429\n",
       "grade_13            2.6722      0.661      4.045      0.000       1.377       3.967\n",
       "floors_1_5          0.2894      0.016     18.085      0.000       0.258       0.321\n",
       "floors_2_0         -0.0999      0.013     -7.807      0.000      -0.125      -0.075\n",
       "floors_2_5          0.1745      0.051      3.422      0.001       0.075       0.274\n",
       "floors_3_0          0.0560      0.029      1.900      0.057      -0.002       0.114\n",
       "floors_3_5          0.0954      0.238      0.401      0.688      -0.371       0.562\n",
       "bath_0_75           0.6567      0.322      2.041      0.041       0.026       1.287\n",
       "bath_1_0            0.5819      0.312      1.863      0.062      -0.030       1.194\n",
       "bath_1_25           0.4886      0.375      1.302      0.193      -0.247       1.224\n",
       "bath_1_5            0.4705      0.313      1.504      0.133      -0.143       1.083\n",
       "bath_1_75           0.5325      0.313      1.704      0.088      -0.080       1.145\n",
       "bath_2_0            0.5153      0.313      1.648      0.099      -0.098       1.128\n",
       "bath_2_25           0.4898      0.313      1.566      0.117      -0.123       1.103\n",
       "bath_2_5            0.3839      0.313      1.228      0.219      -0.229       0.997\n",
       "bath_2_75           0.5118      0.313      1.635      0.102      -0.102       1.125\n",
       "bath_3_0            0.5540      0.313      1.767      0.077      -0.060       1.168\n",
       "bath_3_25           0.6621      0.314      2.110      0.035       0.047       1.277\n",
       "bath_3_5            0.6085      0.314      1.940      0.052      -0.006       1.223\n",
       "bath_3_75           0.8255      0.317      2.605      0.009       0.204       1.447\n",
       "bath_4_0            0.7227      0.318      2.275      0.023       0.100       1.345\n",
       "bath_4_25           0.8537      0.321      2.659      0.008       0.224       1.483\n",
       "bath_4_5            0.6755      0.320      2.114      0.035       0.049       1.302\n",
       "bath_4_75           0.9520      0.340      2.801      0.005       0.286       1.618\n",
       "bath_5_0            0.7683      0.342      2.246      0.025       0.098       1.439\n",
       "bath_5_25           0.9032      0.359      2.512      0.012       0.199       1.608\n",
       "bath_5_5            0.9301      0.373      2.492      0.013       0.198       1.662\n",
       "bath_5_75           0.8507      0.447      1.901      0.057      -0.026       1.728\n",
       "bath_6_0            1.1272      0.409      2.754      0.006       0.325       1.929\n",
       "bath_6_25           1.2227      0.550      2.221      0.026       0.144       2.302\n",
       "bath_6_5            0.4873      0.543      0.898      0.369      -0.577       1.551\n",
       "bath_6_75          -0.0735      0.546     -0.135      0.893      -1.143       0.996\n",
       "bath_7_5           -0.3275      0.752     -0.435      0.663      -1.802       1.147\n",
       "bath_7_75           2.1259      0.726      2.928      0.003       0.703       3.549\n",
       "bath_8_0            1.0100      0.556      1.817      0.069      -0.079       2.099\n",
       "condition_2        -0.1099      0.128     -0.859      0.390      -0.361       0.141\n",
       "condition_3         0.0825      0.119      0.694      0.488      -0.151       0.315\n",
       "condition_4         0.2315      0.119      1.946      0.052      -0.002       0.465\n",
       "condition_5         0.4481      0.120      3.745      0.000       0.214       0.683\n",
       "sqft_living_log     0.4470      0.009     47.208      0.000       0.428       0.466\n",
       "sqft_lot_log       -0.0928      0.005    -17.994      0.000      -0.103      -0.083\n",
       "==============================================================================\n",
       "Omnibus:                        8.233   Durbin-Watson:                   1.983\n",
       "Prob(Omnibus):                  0.016   Jarque-Bera (JB):                8.827\n",
       "Skew:                           0.012   Prob(JB):                       0.0121\n",
       "Kurtosis:                       3.096   Cond. No.                         773.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = preprocessed.drop(['price'], axis = 1)\n",
    "y = preprocessed['price']\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Train Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16065 5355 16065 5355\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed.drop(['price'], axis = 1)\n",
    "y = preprocessed['price']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.612</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.611</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   428.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:18:41</td>     <th>  Log-Likelihood:    </th> <td> -15159.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16065</td>      <th>  AIC:               </th> <td>3.044e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16005</td>      <th>  BIC:               </th> <td>3.090e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    59</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -0.6813</td> <td>    0.740</td> <td>   -0.920</td> <td> 0.357</td> <td>   -2.132</td> <td>    0.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront_1_0</th>  <td>    1.2119</td> <td>    0.063</td> <td>   19.357</td> <td> 0.000</td> <td>    1.089</td> <td>    1.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>   -0.1517</td> <td>    0.056</td> <td>   -2.724</td> <td> 0.006</td> <td>   -0.261</td> <td>   -0.043</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_3</th>           <td>   -0.4296</td> <td>    0.056</td> <td>   -7.679</td> <td> 0.000</td> <td>   -0.539</td> <td>   -0.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_4</th>           <td>   -0.4638</td> <td>    0.057</td> <td>   -8.076</td> <td> 0.000</td> <td>   -0.576</td> <td>   -0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_5</th>           <td>   -0.4887</td> <td>    0.060</td> <td>   -8.078</td> <td> 0.000</td> <td>   -0.607</td> <td>   -0.370</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_6</th>           <td>   -0.5952</td> <td>    0.074</td> <td>   -7.998</td> <td> 0.000</td> <td>   -0.741</td> <td>   -0.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_7</th>           <td>   -0.5202</td> <td>    0.135</td> <td>   -3.867</td> <td> 0.000</td> <td>   -0.784</td> <td>   -0.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_8</th>           <td>   -0.2866</td> <td>    0.211</td> <td>   -1.358</td> <td> 0.175</td> <td>   -0.700</td> <td>    0.127</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_9</th>           <td>    0.0680</td> <td>    0.447</td> <td>    0.152</td> <td> 0.879</td> <td>   -0.809</td> <td>    0.945</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_10</th>          <td>   -0.1463</td> <td>    0.461</td> <td>   -0.317</td> <td> 0.751</td> <td>   -1.051</td> <td>    0.758</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_11</th>          <td>   -0.3166</td> <td>    0.626</td> <td>   -0.506</td> <td> 0.613</td> <td>   -1.544</td> <td>    0.911</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_33</th>          <td> 4.276e-15</td> <td> 2.23e-15</td> <td>    1.921</td> <td> 0.055</td> <td>-8.64e-17</td> <td> 8.64e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_4</th>         <td>   -0.5977</td> <td>    0.643</td> <td>   -0.929</td> <td> 0.353</td> <td>   -1.859</td> <td>    0.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.4079</td> <td>    0.633</td> <td>   -0.644</td> <td> 0.519</td> <td>   -1.649</td> <td>    0.833</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>         <td>   -0.1502</td> <td>    0.632</td> <td>   -0.238</td> <td> 0.812</td> <td>   -1.390</td> <td>    1.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>    0.2198</td> <td>    0.632</td> <td>    0.347</td> <td> 0.728</td> <td>   -1.020</td> <td>    1.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>         <td>    0.6156</td> <td>    0.633</td> <td>    0.973</td> <td> 0.331</td> <td>   -0.624</td> <td>    1.856</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    1.0796</td> <td>    0.633</td> <td>    1.706</td> <td> 0.088</td> <td>   -0.161</td> <td>    2.320</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    1.4301</td> <td>    0.633</td> <td>    2.258</td> <td> 0.024</td> <td>    0.189</td> <td>    2.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    1.7319</td> <td>    0.634</td> <td>    2.730</td> <td> 0.006</td> <td>    0.489</td> <td>    2.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_12</th>        <td>    2.1663</td> <td>    0.639</td> <td>    3.390</td> <td> 0.001</td> <td>    0.914</td> <td>    3.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_13</th>        <td>    2.8544</td> <td>    0.701</td> <td>    4.074</td> <td> 0.000</td> <td>    1.481</td> <td>    4.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.2992</td> <td>    0.018</td> <td>   16.180</td> <td> 0.000</td> <td>    0.263</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.0936</td> <td>    0.015</td> <td>   -6.353</td> <td> 0.000</td> <td>   -0.122</td> <td>   -0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_5</th>      <td>    0.2033</td> <td>    0.058</td> <td>    3.503</td> <td> 0.000</td> <td>    0.090</td> <td>    0.317</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_0</th>      <td>    0.0688</td> <td>    0.034</td> <td>    2.020</td> <td> 0.043</td> <td>    0.002</td> <td>    0.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_5</th>      <td>    0.1089</td> <td>    0.238</td> <td>    0.457</td> <td> 0.648</td> <td>   -0.358</td> <td>    0.576</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_0_75</th>       <td>    0.7590</td> <td>    0.373</td> <td>    2.033</td> <td> 0.042</td> <td>    0.027</td> <td>    1.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.6867</td> <td>    0.360</td> <td>    1.905</td> <td> 0.057</td> <td>   -0.020</td> <td>    1.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_25</th>       <td>    0.6276</td> <td>    0.441</td> <td>    1.423</td> <td> 0.155</td> <td>   -0.237</td> <td>    1.492</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_5</th>        <td>    0.5876</td> <td>    0.361</td> <td>    1.628</td> <td> 0.104</td> <td>   -0.120</td> <td>    1.295</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_75</th>       <td>    0.6361</td> <td>    0.361</td> <td>    1.763</td> <td> 0.078</td> <td>   -0.071</td> <td>    1.343</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_0</th>        <td>    0.6264</td> <td>    0.361</td> <td>    1.736</td> <td> 0.083</td> <td>   -0.081</td> <td>    1.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_25</th>       <td>    0.5985</td> <td>    0.361</td> <td>    1.658</td> <td> 0.097</td> <td>   -0.109</td> <td>    1.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>    0.4939</td> <td>    0.361</td> <td>    1.369</td> <td> 0.171</td> <td>   -0.213</td> <td>    1.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_75</th>       <td>    0.6376</td> <td>    0.361</td> <td>    1.765</td> <td> 0.078</td> <td>   -0.070</td> <td>    1.346</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_0</th>        <td>    0.6428</td> <td>    0.362</td> <td>    1.777</td> <td> 0.076</td> <td>   -0.066</td> <td>    1.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_25</th>       <td>    0.7853</td> <td>    0.362</td> <td>    2.169</td> <td> 0.030</td> <td>    0.076</td> <td>    1.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_5</th>        <td>    0.7133</td> <td>    0.362</td> <td>    1.971</td> <td> 0.049</td> <td>    0.004</td> <td>    1.423</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_75</th>       <td>    0.9243</td> <td>    0.365</td> <td>    2.529</td> <td> 0.011</td> <td>    0.208</td> <td>    1.641</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_0</th>        <td>    0.8333</td> <td>    0.367</td> <td>    2.271</td> <td> 0.023</td> <td>    0.114</td> <td>    1.553</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_25</th>       <td>    1.0136</td> <td>    0.370</td> <td>    2.736</td> <td> 0.006</td> <td>    0.288</td> <td>    1.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_5</th>        <td>    0.7907</td> <td>    0.369</td> <td>    2.141</td> <td> 0.032</td> <td>    0.067</td> <td>    1.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_75</th>       <td>    1.0348</td> <td>    0.393</td> <td>    2.636</td> <td> 0.008</td> <td>    0.265</td> <td>    1.804</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_0</th>        <td>    1.0889</td> <td>    0.398</td> <td>    2.733</td> <td> 0.006</td> <td>    0.308</td> <td>    1.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_25</th>       <td>    0.7865</td> <td>    0.437</td> <td>    1.801</td> <td> 0.072</td> <td>   -0.070</td> <td>    1.643</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_5</th>        <td>    1.0358</td> <td>    0.435</td> <td>    2.382</td> <td> 0.017</td> <td>    0.183</td> <td>    1.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_5_75</th>       <td>    1.2463</td> <td>    0.591</td> <td>    2.108</td> <td> 0.035</td> <td>    0.087</td> <td>    2.405</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_0</th>        <td>    1.9578</td> <td>    0.518</td> <td>    3.780</td> <td> 0.000</td> <td>    0.943</td> <td>    2.973</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_25</th>       <td>    1.9155</td> <td>    0.722</td> <td>    2.655</td> <td> 0.008</td> <td>    0.501</td> <td>    3.330</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_5</th>        <td>    0.6496</td> <td>    0.572</td> <td>    1.135</td> <td> 0.256</td> <td>   -0.472</td> <td>    1.771</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_75</th>       <td> 1.366e-15</td> <td> 1.05e-15</td> <td>    1.299</td> <td> 0.194</td> <td>-6.95e-16</td> <td> 3.43e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_7_5</th>        <td>   -0.3083</td> <td>    0.845</td> <td>   -0.365</td> <td> 0.715</td> <td>   -1.965</td> <td>    1.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_7_75</th>       <td>    2.1116</td> <td>    0.782</td> <td>    2.702</td> <td> 0.007</td> <td>    0.580</td> <td>    3.644</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_8_0</th>        <td>    0.3655</td> <td>    0.737</td> <td>    0.496</td> <td> 0.620</td> <td>   -1.079</td> <td>    1.810</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_2</th>     <td>   -0.2825</td> <td>    0.151</td> <td>   -1.874</td> <td> 0.061</td> <td>   -0.578</td> <td>    0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>     <td>   -0.0920</td> <td>    0.141</td> <td>   -0.654</td> <td> 0.513</td> <td>   -0.368</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.0577</td> <td>    0.141</td> <td>    0.410</td> <td> 0.682</td> <td>   -0.218</td> <td>    0.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.2704</td> <td>    0.142</td> <td>    1.911</td> <td> 0.056</td> <td>   -0.007</td> <td>    0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4525</td> <td>    0.011</td> <td>   41.476</td> <td> 0.000</td> <td>    0.431</td> <td>    0.474</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.0942</td> <td>    0.006</td> <td>  -15.821</td> <td> 0.000</td> <td>   -0.106</td> <td>   -0.083</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.492</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.064</td> <th>  Jarque-Bera (JB):  </th> <td>   5.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.012</td> <th>  Prob(JB):          </th> <td>  0.0547</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.090</td> <th>  Cond. No.          </th> <td>1.33e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The smallest eigenvalue is 2.25e-28. This might indicate that there are<br/>strong multicollinearity problems or that the design matrix is singular."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.612\n",
       "Model:                            OLS   Adj. R-squared:                  0.611\n",
       "Method:                 Least Squares   F-statistic:                     428.0\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:18:41   Log-Likelihood:                -15159.\n",
       "No. Observations:               16065   AIC:                         3.044e+04\n",
       "Df Residuals:                   16005   BIC:                         3.090e+04\n",
       "Df Model:                          59                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -0.6813      0.740     -0.920      0.357      -2.132       0.770\n",
       "waterfront_1_0      1.2119      0.063     19.357      0.000       1.089       1.335\n",
       "bed_2              -0.1517      0.056     -2.724      0.006      -0.261      -0.043\n",
       "bed_3              -0.4296      0.056     -7.679      0.000      -0.539      -0.320\n",
       "bed_4              -0.4638      0.057     -8.076      0.000      -0.576      -0.351\n",
       "bed_5              -0.4887      0.060     -8.078      0.000      -0.607      -0.370\n",
       "bed_6              -0.5952      0.074     -7.998      0.000      -0.741      -0.449\n",
       "bed_7              -0.5202      0.135     -3.867      0.000      -0.784      -0.257\n",
       "bed_8              -0.2866      0.211     -1.358      0.175      -0.700       0.127\n",
       "bed_9               0.0680      0.447      0.152      0.879      -0.809       0.945\n",
       "bed_10             -0.1463      0.461     -0.317      0.751      -1.051       0.758\n",
       "bed_11             -0.3166      0.626     -0.506      0.613      -1.544       0.911\n",
       "bed_33           4.276e-15   2.23e-15      1.921      0.055   -8.64e-17    8.64e-15\n",
       "grade_4            -0.5977      0.643     -0.929      0.353      -1.859       0.664\n",
       "grade_5            -0.4079      0.633     -0.644      0.519      -1.649       0.833\n",
       "grade_6            -0.1502      0.632     -0.238      0.812      -1.390       1.089\n",
       "grade_7             0.2198      0.632      0.347      0.728      -1.020       1.459\n",
       "grade_8             0.6156      0.633      0.973      0.331      -0.624       1.856\n",
       "grade_9             1.0796      0.633      1.706      0.088      -0.161       2.320\n",
       "grade_10            1.4301      0.633      2.258      0.024       0.189       2.671\n",
       "grade_11            1.7319      0.634      2.730      0.006       0.489       2.975\n",
       "grade_12            2.1663      0.639      3.390      0.001       0.914       3.419\n",
       "grade_13            2.8544      0.701      4.074      0.000       1.481       4.228\n",
       "floors_1_5          0.2992      0.018     16.180      0.000       0.263       0.335\n",
       "floors_2_0         -0.0936      0.015     -6.353      0.000      -0.122      -0.065\n",
       "floors_2_5          0.2033      0.058      3.503      0.000       0.090       0.317\n",
       "floors_3_0          0.0688      0.034      2.020      0.043       0.002       0.136\n",
       "floors_3_5          0.1089      0.238      0.457      0.648      -0.358       0.576\n",
       "bath_0_75           0.7590      0.373      2.033      0.042       0.027       1.491\n",
       "bath_1_0            0.6867      0.360      1.905      0.057      -0.020       1.393\n",
       "bath_1_25           0.6276      0.441      1.423      0.155      -0.237       1.492\n",
       "bath_1_5            0.5876      0.361      1.628      0.104      -0.120       1.295\n",
       "bath_1_75           0.6361      0.361      1.763      0.078      -0.071       1.343\n",
       "bath_2_0            0.6264      0.361      1.736      0.083      -0.081       1.334\n",
       "bath_2_25           0.5985      0.361      1.658      0.097      -0.109       1.306\n",
       "bath_2_5            0.4939      0.361      1.369      0.171      -0.213       1.201\n",
       "bath_2_75           0.6376      0.361      1.765      0.078      -0.070       1.346\n",
       "bath_3_0            0.6428      0.362      1.777      0.076      -0.066       1.352\n",
       "bath_3_25           0.7853      0.362      2.169      0.030       0.076       1.495\n",
       "bath_3_5            0.7133      0.362      1.971      0.049       0.004       1.423\n",
       "bath_3_75           0.9243      0.365      2.529      0.011       0.208       1.641\n",
       "bath_4_0            0.8333      0.367      2.271      0.023       0.114       1.553\n",
       "bath_4_25           1.0136      0.370      2.736      0.006       0.288       1.740\n",
       "bath_4_5            0.7907      0.369      2.141      0.032       0.067       1.514\n",
       "bath_4_75           1.0348      0.393      2.636      0.008       0.265       1.804\n",
       "bath_5_0            1.0889      0.398      2.733      0.006       0.308       1.870\n",
       "bath_5_25           0.7865      0.437      1.801      0.072      -0.070       1.643\n",
       "bath_5_5            1.0358      0.435      2.382      0.017       0.183       1.888\n",
       "bath_5_75           1.2463      0.591      2.108      0.035       0.087       2.405\n",
       "bath_6_0            1.9578      0.518      3.780      0.000       0.943       2.973\n",
       "bath_6_25           1.9155      0.722      2.655      0.008       0.501       3.330\n",
       "bath_6_5            0.6496      0.572      1.135      0.256      -0.472       1.771\n",
       "bath_6_75        1.366e-15   1.05e-15      1.299      0.194   -6.95e-16    3.43e-15\n",
       "bath_7_5           -0.3083      0.845     -0.365      0.715      -1.965       1.349\n",
       "bath_7_75           2.1116      0.782      2.702      0.007       0.580       3.644\n",
       "bath_8_0            0.3655      0.737      0.496      0.620      -1.079       1.810\n",
       "condition_2        -0.2825      0.151     -1.874      0.061      -0.578       0.013\n",
       "condition_3        -0.0920      0.141     -0.654      0.513      -0.368       0.184\n",
       "condition_4         0.0577      0.141      0.410      0.682      -0.218       0.334\n",
       "condition_5         0.2704      0.142      1.911      0.056      -0.007       0.548\n",
       "sqft_living_log     0.4525      0.011     41.476      0.000       0.431       0.474\n",
       "sqft_lot_log       -0.0942      0.006    -15.821      0.000      -0.106      -0.083\n",
       "==============================================================================\n",
       "Omnibus:                        5.492   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.064   Jarque-Bera (JB):                5.812\n",
       "Skew:                           0.012   Prob(JB):                       0.0547\n",
       "Kurtosis:                       3.090   Cond. No.                     1.33e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The smallest eigenvalue is 2.25e-28. This might indicate that there are\n",
       "strong multicollinearity problems or that the design matrix is singular.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "trainmodel = sm.OLS(y, X).fit()\n",
    "predictions = trainmodel.predict(X) \n",
    "\n",
    "trainmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "preds = linreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.21185930e+00, -1.53462055e-01, -4.30886963e-01, -4.65098635e-01,\n",
       "       -4.89815877e-01, -5.97166760e-01, -5.20201823e-01, -2.93397190e-01,\n",
       "        6.42849926e-02, -1.49042562e-01, -3.18277412e-01,  2.66683181e+12,\n",
       "       -6.01004634e-01, -4.06993463e-01, -1.49429914e-01,  2.21670199e-01,\n",
       "        6.17548187e-01,  1.08164960e+00,  1.43204557e+00,  1.73072942e+00,\n",
       "        2.16651884e+00,  2.85734618e+00,  2.98830484e-01, -9.37954406e-02,\n",
       "        2.00821631e-01,  7.15846993e-02,  1.11126712e-01,  7.56411769e-01,\n",
       "        6.87444255e-01,  6.30906318e-01,  5.88099084e-01,  6.35969299e-01,\n",
       "        6.25948381e-01,  5.97682727e-01,  4.93087801e-01,  6.36607541e-01,\n",
       "        6.40868425e-01,  7.84513038e-01,  7.11698690e-01,  9.20989310e-01,\n",
       "        8.33311933e-01,  1.01726855e+00,  7.95432355e-01,  1.04257269e+00,\n",
       "        1.09085373e+00,  7.85304426e-01,  1.03415480e+00,  1.24981649e+00,\n",
       "        1.94337498e+00,  1.91810528e+00,  6.54305193e-01, -1.75443293e+08,\n",
       "       -3.07259299e-01,  2.11186177e+00,  3.68817410e-01, -2.79055809e-01,\n",
       "       -9.07452825e-02,  5.84530292e-02,  2.72684671e-01,  4.52950866e-01,\n",
       "       -9.41271740e-02])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6824648898344207"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6120389713606142"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" \n",
    "    Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like with the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype='float64')\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  sqft_living_log                with p-value 0.0\n",
      "Add  grade_7                        with p-value 1.85158e-107\n",
      "Add  grade_6                        with p-value 1.05157e-101\n",
      "Add  grade_8                        with p-value 8.99495e-170\n",
      "Add  grade_5                        with p-value 1.35861e-117\n",
      "Add  bath_2_5                       with p-value 5.24117e-107\n",
      "Add  waterfront_1_0                 with p-value 4.03935e-101\n",
      "Add  floors_1_5                     with p-value 6.33074e-72\n",
      "Add  grade_9                        with p-value 3.28756e-78\n",
      "Add  bed_2                          with p-value 1.56757e-63\n",
      "Add  condition_5                    with p-value 1.99783e-60\n",
      "Add  sqft_lot_log                   with p-value 1.94488e-53\n",
      "Add  condition_4                    with p-value 9.60314e-37\n",
      "Add  grade_4                        with p-value 9.2125e-37\n",
      "Add  grade_10                       with p-value 3.90709e-32\n",
      "Add  floors_2_0                     with p-value 6.02599e-15\n",
      "Add  grade_11                       with p-value 3.88448e-11\n",
      "Add  bath_1_0                       with p-value 1.21885e-05\n",
      "Add  bath_3_75                      with p-value 0.000129554\n",
      "Add  bath_3_25                      with p-value 0.000112414\n",
      "Add  bath_4_25                      with p-value 5.1185e-05\n",
      "Add  grade_13                       with p-value 0.000281437\n",
      "Add  floors_2_5                     with p-value 0.000452997\n",
      "Add  bath_6_0                       with p-value 0.000501666\n",
      "Add  condition_2                    with p-value 0.000789265\n",
      "Add  grade_12                       with p-value 0.0050749\n",
      "Drop grade_8                        with p-value 0.881547\n",
      "resulting features:\n",
      "['sqft_living_log', 'grade_7', 'grade_6', 'grade_5', 'bath_2_5', 'waterfront_1_0', 'floors_1_5', 'grade_9', 'bed_2', 'condition_5', 'sqft_lot_log', 'condition_4', 'grade_4', 'grade_10', 'floors_2_0', 'grade_11', 'bath_1_0', 'bath_3_75', 'bath_3_25', 'bath_4_25', 'grade_13', 'floors_2_5', 'bath_6_0', 'condition_2', 'grade_12']\n"
     ]
    }
   ],
   "source": [
    "result = stepwise_selection(X_train, y_train, verbose = True)\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.609</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.608</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   997.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:19:19</td>     <th>  Log-Likelihood:    </th> <td> -15228.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 16065</td>      <th>  AIC:               </th> <td>3.051e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 16039</td>      <th>  BIC:               </th> <td>3.071e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    25</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>    0.0415</td> <td>    0.013</td> <td>    3.203</td> <td> 0.001</td> <td>    0.016</td> <td>    0.067</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4374</td> <td>    0.009</td> <td>   47.723</td> <td> 0.000</td> <td>    0.419</td> <td>    0.455</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>   -0.4082</td> <td>    0.013</td> <td>  -30.429</td> <td> 0.000</td> <td>   -0.435</td> <td>   -0.382</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>         <td>   -0.7687</td> <td>    0.023</td> <td>  -33.324</td> <td> 0.000</td> <td>   -0.814</td> <td>   -0.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.9840</td> <td>    0.051</td> <td>  -19.414</td> <td> 0.000</td> <td>   -1.083</td> <td>   -0.885</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>   -0.1421</td> <td>    0.013</td> <td>  -10.768</td> <td> 0.000</td> <td>   -0.168</td> <td>   -0.116</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>waterfront_1_0</th>  <td>    1.2520</td> <td>    0.063</td> <td>   20.026</td> <td> 0.000</td> <td>    1.129</td> <td>    1.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.2872</td> <td>    0.018</td> <td>   15.648</td> <td> 0.000</td> <td>    0.251</td> <td>    0.323</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    0.4775</td> <td>    0.018</td> <td>   26.613</td> <td> 0.000</td> <td>    0.442</td> <td>    0.513</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>    0.2588</td> <td>    0.017</td> <td>   15.122</td> <td> 0.000</td> <td>    0.225</td> <td>    0.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.3557</td> <td>    0.019</td> <td>   18.358</td> <td> 0.000</td> <td>    0.318</td> <td>    0.394</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.0988</td> <td>    0.006</td> <td>  -17.586</td> <td> 0.000</td> <td>   -0.110</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.1391</td> <td>    0.012</td> <td>   11.593</td> <td> 0.000</td> <td>    0.116</td> <td>    0.163</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_4</th>         <td>   -0.9961</td> <td>    0.149</td> <td>   -6.676</td> <td> 0.000</td> <td>   -1.289</td> <td>   -0.704</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    0.8534</td> <td>    0.026</td> <td>   33.281</td> <td> 0.000</td> <td>    0.803</td> <td>    0.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.1001</td> <td>    0.013</td> <td>   -7.438</td> <td> 0.000</td> <td>   -0.126</td> <td>   -0.074</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    1.2058</td> <td>    0.041</td> <td>   29.205</td> <td> 0.000</td> <td>    1.125</td> <td>    1.287</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.0704</td> <td>    0.017</td> <td>    4.054</td> <td> 0.000</td> <td>    0.036</td> <td>    0.105</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_75</th>       <td>    0.2490</td> <td>    0.058</td> <td>    4.263</td> <td> 0.000</td> <td>    0.134</td> <td>    0.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_25</th>       <td>    0.1307</td> <td>    0.032</td> <td>    4.080</td> <td> 0.000</td> <td>    0.068</td> <td>    0.194</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_25</th>       <td>    0.3205</td> <td>    0.083</td> <td>    3.846</td> <td> 0.000</td> <td>    0.157</td> <td>    0.484</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_13</th>        <td>    2.6701</td> <td>    0.257</td> <td>   10.404</td> <td> 0.000</td> <td>    2.167</td> <td>    3.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_5</th>      <td>    0.2032</td> <td>    0.058</td> <td>    3.520</td> <td> 0.000</td> <td>    0.090</td> <td>    0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_6_0</th>        <td>    1.2555</td> <td>    0.365</td> <td>    3.436</td> <td> 0.001</td> <td>    0.539</td> <td>    1.972</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_2</th>     <td>   -0.1913</td> <td>    0.057</td> <td>   -3.358</td> <td> 0.001</td> <td>   -0.303</td> <td>   -0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_12</th>        <td>    1.6787</td> <td>    0.085</td> <td>   19.849</td> <td> 0.000</td> <td>    1.513</td> <td>    1.845</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.129</td> <th>  Durbin-Watson:     </th> <td>   1.993</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.077</td> <th>  Jarque-Bera (JB):  </th> <td>   5.393</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.013</td> <th>  Prob(JB):          </th> <td>  0.0674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.086</td> <th>  Cond. No.          </th> <td>    93.6</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.609\n",
       "Model:                            OLS   Adj. R-squared:                  0.608\n",
       "Method:                 Least Squares   F-statistic:                     997.9\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:19:19   Log-Likelihood:                -15228.\n",
       "No. Observations:               16065   AIC:                         3.051e+04\n",
       "Df Residuals:                   16039   BIC:                         3.071e+04\n",
       "Df Model:                          25                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const               0.0415      0.013      3.203      0.001       0.016       0.067\n",
       "sqft_living_log     0.4374      0.009     47.723      0.000       0.419       0.455\n",
       "grade_7            -0.4082      0.013    -30.429      0.000      -0.435      -0.382\n",
       "grade_6            -0.7687      0.023    -33.324      0.000      -0.814      -0.723\n",
       "grade_5            -0.9840      0.051    -19.414      0.000      -1.083      -0.885\n",
       "bath_2_5           -0.1421      0.013    -10.768      0.000      -0.168      -0.116\n",
       "waterfront_1_0      1.2520      0.063     20.026      0.000       1.129       1.375\n",
       "floors_1_5          0.2872      0.018     15.648      0.000       0.251       0.323\n",
       "grade_9             0.4775      0.018     26.613      0.000       0.442       0.513\n",
       "bed_2               0.2588      0.017     15.122      0.000       0.225       0.292\n",
       "condition_5         0.3557      0.019     18.358      0.000       0.318       0.394\n",
       "sqft_lot_log       -0.0988      0.006    -17.586      0.000      -0.110      -0.088\n",
       "condition_4         0.1391      0.012     11.593      0.000       0.116       0.163\n",
       "grade_4            -0.9961      0.149     -6.676      0.000      -1.289      -0.704\n",
       "grade_10            0.8534      0.026     33.281      0.000       0.803       0.904\n",
       "floors_2_0         -0.1001      0.013     -7.438      0.000      -0.126      -0.074\n",
       "grade_11            1.2058      0.041     29.205      0.000       1.125       1.287\n",
       "bath_1_0            0.0704      0.017      4.054      0.000       0.036       0.105\n",
       "bath_3_75           0.2490      0.058      4.263      0.000       0.134       0.363\n",
       "bath_3_25           0.1307      0.032      4.080      0.000       0.068       0.194\n",
       "bath_4_25           0.3205      0.083      3.846      0.000       0.157       0.484\n",
       "grade_13            2.6701      0.257     10.404      0.000       2.167       3.173\n",
       "floors_2_5          0.2032      0.058      3.520      0.000       0.090       0.316\n",
       "bath_6_0            1.2555      0.365      3.436      0.001       0.539       1.972\n",
       "condition_2        -0.1913      0.057     -3.358      0.001      -0.303      -0.080\n",
       "grade_12            1.6787      0.085     19.849      0.000       1.513       1.845\n",
       "==============================================================================\n",
       "Omnibus:                        5.129   Durbin-Watson:                   1.993\n",
       "Prob(Omnibus):                  0.077   Jarque-Bera (JB):                5.393\n",
       "Skew:                           0.013   Prob(JB):                       0.0674\n",
       "Kurtosis:                       3.086   Cond. No.                         93.6\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fin = X_train[result]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model = sm.OLS(y_train,X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3281030711910586e+21"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "test_residuals = y_hat_test - y_test\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "cv_5_results = cross_val_score(linreg, X, y, cv=5, scoring=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.04016870e-01, 2.37593624e+18, 1.70989913e+15, 3.90556569e-01,\n",
       "       4.68942940e+21])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.383614100116286e+20"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OK so that's a huge MSE, meaning we still have some fine tuning to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([    5,    21,    36, ..., 21356, 21384, 21384], dtype=int64), array([4, 1, 8, ..., 5, 1, 7], dtype=int64))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>yr_built</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>263000018</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>291310100</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20547 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id     price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "0      7129300520  221900.0         3       1.00         1180      5650   \n",
       "1      6414100192  538000.0         3       2.25         2570      7242   \n",
       "2      5631500400  180000.0         2       1.00          770     10000   \n",
       "3      2487200875  604000.0         4       3.00         1960      5000   \n",
       "4      1954400510  510000.0         3       2.00         1680      8080   \n",
       "...           ...       ...       ...        ...          ...       ...   \n",
       "21592   263000018  360000.0         3       2.50         1530      1131   \n",
       "21593  6600060120  400000.0         4       2.50         2310      5813   \n",
       "21594  1523300141  402101.0         2       0.75         1020      1350   \n",
       "21595   291310100  400000.0         3       2.50         1600      2388   \n",
       "21596  1523300157  325000.0         2       0.75         1020      1076   \n",
       "\n",
       "       floors  waterfront  condition  grade  yr_built  \n",
       "0         1.0         0.0          3      7      1955  \n",
       "1         2.0         0.0          3      7      1951  \n",
       "2         1.0         0.0          3      6      1933  \n",
       "3         1.0         0.0          5      7      1965  \n",
       "4         1.0         0.0          3      8      1987  \n",
       "...       ...         ...        ...    ...       ...  \n",
       "21592     3.0         0.0          3      8      2009  \n",
       "21593     2.0         0.0          3      8      2014  \n",
       "21594     2.0         0.0          3      7      2009  \n",
       "21595     2.0         0.0          3      8      2004  \n",
       "21596     2.0         0.0          3      7      2008  \n",
       "\n",
       "[20547 rows x 11 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "z = np.abs(stats.zscore(kc_new))\n",
    "z\n",
    "\n",
    "threshold = 3.5\n",
    "print(np.where(z >= 3.5))\n",
    "\n",
    "kc_new_out = kc_new[(z<=3.5).all(axis = 1)]\n",
    "kc_new_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous = ['sqft_living', 'sqft_lot']\n",
    "categoricals = ['waterfront', 'bedrooms', 'floors', 'grade', 'bathrooms', 'condition']\n",
    "kccat = kc_new_out[categoricals]\n",
    "kccon = kc_new_out[continuous]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_ohe = pd.get_dummies(kccat['condition'], prefix = 'condition', drop_first=True)\n",
    "waterfront_ohe =pd.get_dummies(kccat['waterfront'], prefix = 'waterfront', drop_first=True)\n",
    "grade_ohe= pd.get_dummies(kccat['grade'], prefix = 'grade', drop_first=True)\n",
    "bed_ohe= pd.get_dummies(kccat['bedrooms'], prefix = 'bed', drop_first=True)\n",
    "bath_ohe= pd.get_dummies(kccat['bathrooms'], prefix = 'bath', drop_first=True)\n",
    "floors_ohe= pd.get_dummies(kccat['floors'], prefix = 'floors', drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bed_2</th>\n",
       "      <th>bed_3</th>\n",
       "      <th>bed_4</th>\n",
       "      <th>bed_5</th>\n",
       "      <th>bed_6</th>\n",
       "      <th>floors_1.5</th>\n",
       "      <th>floors_2.0</th>\n",
       "      <th>floors_2.5</th>\n",
       "      <th>floors_3.0</th>\n",
       "      <th>bath_0.75</th>\n",
       "      <th>...</th>\n",
       "      <th>grade_5</th>\n",
       "      <th>grade_6</th>\n",
       "      <th>grade_7</th>\n",
       "      <th>grade_8</th>\n",
       "      <th>grade_9</th>\n",
       "      <th>grade_10</th>\n",
       "      <th>grade_11</th>\n",
       "      <th>condition_3</th>\n",
       "      <th>condition_4</th>\n",
       "      <th>condition_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20547 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       bed_2  bed_3  bed_4  bed_5  bed_6  floors_1.5  floors_2.0  floors_2.5  \\\n",
       "0          0      1      0      0      0           0           0           0   \n",
       "1          0      1      0      0      0           0           1           0   \n",
       "2          1      0      0      0      0           0           0           0   \n",
       "3          0      0      1      0      0           0           0           0   \n",
       "4          0      1      0      0      0           0           0           0   \n",
       "...      ...    ...    ...    ...    ...         ...         ...         ...   \n",
       "21592      0      1      0      0      0           0           0           0   \n",
       "21593      0      0      1      0      0           0           1           0   \n",
       "21594      1      0      0      0      0           0           1           0   \n",
       "21595      0      1      0      0      0           0           1           0   \n",
       "21596      1      0      0      0      0           0           1           0   \n",
       "\n",
       "       floors_3.0  bath_0.75  ...  grade_5  grade_6  grade_7  grade_8  \\\n",
       "0               0          0  ...        0        0        1        0   \n",
       "1               0          0  ...        0        0        1        0   \n",
       "2               0          0  ...        0        1        0        0   \n",
       "3               0          0  ...        0        0        1        0   \n",
       "4               0          0  ...        0        0        0        1   \n",
       "...           ...        ...  ...      ...      ...      ...      ...   \n",
       "21592           1          0  ...        0        0        0        1   \n",
       "21593           0          0  ...        0        0        0        1   \n",
       "21594           0          1  ...        0        0        1        0   \n",
       "21595           0          0  ...        0        0        0        1   \n",
       "21596           0          1  ...        0        0        1        0   \n",
       "\n",
       "       grade_9  grade_10  grade_11  condition_3  condition_4  condition_5  \n",
       "0            0         0         0            1            0            0  \n",
       "1            0         0         0            1            0            0  \n",
       "2            0         0         0            1            0            0  \n",
       "3            0         0         0            0            0            1  \n",
       "4            0         0         0            1            0            0  \n",
       "...        ...       ...       ...          ...          ...          ...  \n",
       "21592        0         0         0            1            0            0  \n",
       "21593        0         0         0            1            0            0  \n",
       "21594        0         0         0            1            0            0  \n",
       "21595        0         0         0            1            0            0  \n",
       "21596        0         0         0            1            0            0  \n",
       "\n",
       "[20547 rows x 36 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_concat = pd.concat([waterfront_ohe, bed_ohe, floors_ohe, bath_ohe, grade_ohe, condition_ohe], axis = 1)\n",
    "ohe_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1.451898\n",
       "1        0.373872\n",
       "2       -1.883318\n",
       "3        0.612425\n",
       "4        0.263687\n",
       "           ...   \n",
       "21592   -0.454363\n",
       "21593   -0.237158\n",
       "21594   -0.226358\n",
       "21595   -0.237158\n",
       "21596   -0.665216\n",
       "Name: price, Length: 20547, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price_log = np.log(kc_new_out['price'])\n",
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "price_log_norm = normalize(price_log)\n",
    "price_log_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sqft_living_log', 'sqft_lot_log']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log and normalize features\n",
    "log_names = [f'{column}_log' for column in kccon.columns]\n",
    "\n",
    "kccon_log = np.log(kccon)\n",
    "kccon_log.columns = log_names\n",
    "\n",
    "log_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(feature):\n",
    "    return (feature - feature.mean()) / feature.std()\n",
    "\n",
    "kc_log_norm = kccon_log.apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed2 = pd.concat([ohe_concat, kc_log_norm, price_log_norm], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed2.columns = preprocessed2.columns.str.replace('.','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20547 entries, 0 to 21596\n",
      "Data columns (total 39 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   bed_2            20547 non-null  uint8  \n",
      " 1   bed_3            20547 non-null  uint8  \n",
      " 2   bed_4            20547 non-null  uint8  \n",
      " 3   bed_5            20547 non-null  uint8  \n",
      " 4   bed_6            20547 non-null  uint8  \n",
      " 5   floors_1_5       20547 non-null  uint8  \n",
      " 6   floors_2_0       20547 non-null  uint8  \n",
      " 7   floors_2_5       20547 non-null  uint8  \n",
      " 8   floors_3_0       20547 non-null  uint8  \n",
      " 9   bath_0_75        20547 non-null  uint8  \n",
      " 10  bath_1_0         20547 non-null  uint8  \n",
      " 11  bath_1_25        20547 non-null  uint8  \n",
      " 12  bath_1_5         20547 non-null  uint8  \n",
      " 13  bath_1_75        20547 non-null  uint8  \n",
      " 14  bath_2_0         20547 non-null  uint8  \n",
      " 15  bath_2_25        20547 non-null  uint8  \n",
      " 16  bath_2_5         20547 non-null  uint8  \n",
      " 17  bath_2_75        20547 non-null  uint8  \n",
      " 18  bath_3_0         20547 non-null  uint8  \n",
      " 19  bath_3_25        20547 non-null  uint8  \n",
      " 20  bath_3_5         20547 non-null  uint8  \n",
      " 21  bath_3_75        20547 non-null  uint8  \n",
      " 22  bath_4_0         20547 non-null  uint8  \n",
      " 23  bath_4_25        20547 non-null  uint8  \n",
      " 24  bath_4_5         20547 non-null  uint8  \n",
      " 25  bath_4_75        20547 non-null  uint8  \n",
      " 26  grade_5          20547 non-null  uint8  \n",
      " 27  grade_6          20547 non-null  uint8  \n",
      " 28  grade_7          20547 non-null  uint8  \n",
      " 29  grade_8          20547 non-null  uint8  \n",
      " 30  grade_9          20547 non-null  uint8  \n",
      " 31  grade_10         20547 non-null  uint8  \n",
      " 32  grade_11         20547 non-null  uint8  \n",
      " 33  condition_3      20547 non-null  uint8  \n",
      " 34  condition_4      20547 non-null  uint8  \n",
      " 35  condition_5      20547 non-null  uint8  \n",
      " 36  sqft_living_log  20547 non-null  float64\n",
      " 37  sqft_lot_log     20547 non-null  float64\n",
      " 38  price            20547 non-null  float64\n",
      "dtypes: float64(3), uint8(36)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "preprocessed2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.556</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.555</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   674.8</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:21:09</td>     <th>  Log-Likelihood:    </th> <td> -20822.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 20547</td>      <th>  AIC:               </th> <td>4.172e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 20508</td>      <th>  BIC:               </th> <td>4.203e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    38</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -1.3718</td> <td>    0.366</td> <td>   -3.744</td> <td> 0.000</td> <td>   -2.090</td> <td>   -0.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>   -0.1493</td> <td>    0.054</td> <td>   -2.783</td> <td> 0.005</td> <td>   -0.254</td> <td>   -0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_3</th>           <td>   -0.4418</td> <td>    0.054</td> <td>   -8.189</td> <td> 0.000</td> <td>   -0.548</td> <td>   -0.336</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_4</th>           <td>   -0.4810</td> <td>    0.055</td> <td>   -8.690</td> <td> 0.000</td> <td>   -0.590</td> <td>   -0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_5</th>           <td>   -0.4892</td> <td>    0.058</td> <td>   -8.390</td> <td> 0.000</td> <td>   -0.603</td> <td>   -0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_6</th>           <td>   -0.4952</td> <td>    0.072</td> <td>   -6.862</td> <td> 0.000</td> <td>   -0.637</td> <td>   -0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.3121</td> <td>    0.018</td> <td>   17.775</td> <td> 0.000</td> <td>    0.278</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.1216</td> <td>    0.014</td> <td>   -8.566</td> <td> 0.000</td> <td>   -0.149</td> <td>   -0.094</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_5</th>      <td>    0.1508</td> <td>    0.060</td> <td>    2.515</td> <td> 0.012</td> <td>    0.033</td> <td>    0.268</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_0</th>      <td>    0.0173</td> <td>    0.033</td> <td>    0.531</td> <td> 0.595</td> <td>   -0.047</td> <td>    0.081</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_0_75</th>       <td>    0.7698</td> <td>    0.345</td> <td>    2.229</td> <td> 0.026</td> <td>    0.093</td> <td>    1.447</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.6415</td> <td>    0.334</td> <td>    1.920</td> <td> 0.055</td> <td>   -0.013</td> <td>    1.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_25</th>       <td>    0.5607</td> <td>    0.409</td> <td>    1.371</td> <td> 0.170</td> <td>   -0.241</td> <td>    1.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_5</th>        <td>    0.5108</td> <td>    0.335</td> <td>    1.526</td> <td> 0.127</td> <td>   -0.145</td> <td>    1.167</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_75</th>       <td>    0.5839</td> <td>    0.334</td> <td>    1.746</td> <td> 0.081</td> <td>   -0.072</td> <td>    1.239</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_0</th>        <td>    0.5662</td> <td>    0.335</td> <td>    1.692</td> <td> 0.091</td> <td>   -0.090</td> <td>    1.222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_25</th>       <td>    0.5375</td> <td>    0.335</td> <td>    1.606</td> <td> 0.108</td> <td>   -0.118</td> <td>    1.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>    0.4240</td> <td>    0.335</td> <td>    1.267</td> <td> 0.205</td> <td>   -0.232</td> <td>    1.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_75</th>       <td>    0.5583</td> <td>    0.335</td> <td>    1.667</td> <td> 0.096</td> <td>   -0.098</td> <td>    1.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_0</th>        <td>    0.5863</td> <td>    0.335</td> <td>    1.748</td> <td> 0.081</td> <td>   -0.071</td> <td>    1.244</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_25</th>       <td>    0.6754</td> <td>    0.336</td> <td>    2.010</td> <td> 0.044</td> <td>    0.017</td> <td>    1.334</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_5</th>        <td>    0.6640</td> <td>    0.336</td> <td>    1.978</td> <td> 0.048</td> <td>    0.006</td> <td>    1.322</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_75</th>       <td>    0.8221</td> <td>    0.341</td> <td>    2.414</td> <td> 0.016</td> <td>    0.155</td> <td>    1.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_0</th>        <td>    0.7060</td> <td>    0.343</td> <td>    2.061</td> <td> 0.039</td> <td>    0.034</td> <td>    1.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_25</th>       <td>    0.7888</td> <td>    0.351</td> <td>    2.246</td> <td> 0.025</td> <td>    0.100</td> <td>    1.477</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_5</th>        <td>    0.6204</td> <td>    0.346</td> <td>    1.794</td> <td> 0.073</td> <td>   -0.057</td> <td>    1.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_75</th>       <td>    0.6652</td> <td>    0.432</td> <td>    1.540</td> <td> 0.124</td> <td>   -0.182</td> <td>    1.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.0243</td> <td>    0.145</td> <td>   -0.168</td> <td> 0.867</td> <td>   -0.308</td> <td>    0.259</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>         <td>    0.2804</td> <td>    0.140</td> <td>    2.002</td> <td> 0.045</td> <td>    0.006</td> <td>    0.555</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>    0.6835</td> <td>    0.140</td> <td>    4.869</td> <td> 0.000</td> <td>    0.408</td> <td>    0.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>         <td>    1.1222</td> <td>    0.141</td> <td>    7.956</td> <td> 0.000</td> <td>    0.846</td> <td>    1.399</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    1.6251</td> <td>    0.142</td> <td>   11.440</td> <td> 0.000</td> <td>    1.347</td> <td>    1.904</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    1.9757</td> <td>    0.144</td> <td>   13.757</td> <td> 0.000</td> <td>    1.694</td> <td>    2.257</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    2.2367</td> <td>    0.149</td> <td>   15.020</td> <td> 0.000</td> <td>    1.945</td> <td>    2.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>     <td>    0.2277</td> <td>    0.055</td> <td>    4.150</td> <td> 0.000</td> <td>    0.120</td> <td>    0.335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.3925</td> <td>    0.055</td> <td>    7.122</td> <td> 0.000</td> <td>    0.284</td> <td>    0.501</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.6165</td> <td>    0.057</td> <td>   10.827</td> <td> 0.000</td> <td>    0.505</td> <td>    0.728</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4613</td> <td>    0.010</td> <td>   46.503</td> <td> 0.000</td> <td>    0.442</td> <td>    0.481</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.1151</td> <td>    0.006</td> <td>  -19.904</td> <td> 0.000</td> <td>   -0.126</td> <td>   -0.104</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 6.517</td> <th>  Durbin-Watson:     </th> <td>   1.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.038</td> <th>  Jarque-Bera (JB):  </th> <td>   6.697</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.026</td> <th>  Prob(JB):          </th> <td>  0.0351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.071</td> <th>  Cond. No.          </th> <td>    482.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.556\n",
       "Model:                            OLS   Adj. R-squared:                  0.555\n",
       "Method:                 Least Squares   F-statistic:                     674.8\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:21:09   Log-Likelihood:                -20822.\n",
       "No. Observations:               20547   AIC:                         4.172e+04\n",
       "Df Residuals:                   20508   BIC:                         4.203e+04\n",
       "Df Model:                          38                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -1.3718      0.366     -3.744      0.000      -2.090      -0.654\n",
       "bed_2              -0.1493      0.054     -2.783      0.005      -0.254      -0.044\n",
       "bed_3              -0.4418      0.054     -8.189      0.000      -0.548      -0.336\n",
       "bed_4              -0.4810      0.055     -8.690      0.000      -0.590      -0.373\n",
       "bed_5              -0.4892      0.058     -8.390      0.000      -0.603      -0.375\n",
       "bed_6              -0.4952      0.072     -6.862      0.000      -0.637      -0.354\n",
       "floors_1_5          0.3121      0.018     17.775      0.000       0.278       0.347\n",
       "floors_2_0         -0.1216      0.014     -8.566      0.000      -0.149      -0.094\n",
       "floors_2_5          0.1508      0.060      2.515      0.012       0.033       0.268\n",
       "floors_3_0          0.0173      0.033      0.531      0.595      -0.047       0.081\n",
       "bath_0_75           0.7698      0.345      2.229      0.026       0.093       1.447\n",
       "bath_1_0            0.6415      0.334      1.920      0.055      -0.013       1.297\n",
       "bath_1_25           0.5607      0.409      1.371      0.170      -0.241       1.362\n",
       "bath_1_5            0.5108      0.335      1.526      0.127      -0.145       1.167\n",
       "bath_1_75           0.5839      0.334      1.746      0.081      -0.072       1.239\n",
       "bath_2_0            0.5662      0.335      1.692      0.091      -0.090       1.222\n",
       "bath_2_25           0.5375      0.335      1.606      0.108      -0.118       1.193\n",
       "bath_2_5            0.4240      0.335      1.267      0.205      -0.232       1.080\n",
       "bath_2_75           0.5583      0.335      1.667      0.096      -0.098       1.215\n",
       "bath_3_0            0.5863      0.335      1.748      0.081      -0.071       1.244\n",
       "bath_3_25           0.6754      0.336      2.010      0.044       0.017       1.334\n",
       "bath_3_5            0.6640      0.336      1.978      0.048       0.006       1.322\n",
       "bath_3_75           0.8221      0.341      2.414      0.016       0.155       1.490\n",
       "bath_4_0            0.7060      0.343      2.061      0.039       0.034       1.378\n",
       "bath_4_25           0.7888      0.351      2.246      0.025       0.100       1.477\n",
       "bath_4_5            0.6204      0.346      1.794      0.073      -0.057       1.298\n",
       "bath_4_75           0.6652      0.432      1.540      0.124      -0.182       1.512\n",
       "grade_5            -0.0243      0.145     -0.168      0.867      -0.308       0.259\n",
       "grade_6             0.2804      0.140      2.002      0.045       0.006       0.555\n",
       "grade_7             0.6835      0.140      4.869      0.000       0.408       0.959\n",
       "grade_8             1.1222      0.141      7.956      0.000       0.846       1.399\n",
       "grade_9             1.6251      0.142     11.440      0.000       1.347       1.904\n",
       "grade_10            1.9757      0.144     13.757      0.000       1.694       2.257\n",
       "grade_11            2.2367      0.149     15.020      0.000       1.945       2.529\n",
       "condition_3         0.2277      0.055      4.150      0.000       0.120       0.335\n",
       "condition_4         0.3925      0.055      7.122      0.000       0.284       0.501\n",
       "condition_5         0.6165      0.057     10.827      0.000       0.505       0.728\n",
       "sqft_living_log     0.4613      0.010     46.503      0.000       0.442       0.481\n",
       "sqft_lot_log       -0.1151      0.006    -19.904      0.000      -0.126      -0.104\n",
       "==============================================================================\n",
       "Omnibus:                        6.517   Durbin-Watson:                   1.976\n",
       "Prob(Omnibus):                  0.038   Jarque-Bera (JB):                6.697\n",
       "Skew:                          -0.026   Prob(JB):                       0.0351\n",
       "Kurtosis:                       3.071   Cond. No.                         482.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = preprocessed2.drop(['price'], axis = 1)\n",
    "y = preprocessed2['price']\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15410 5137 15410 5137\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed2.drop(['price'], axis = 1)\n",
    "y = preprocessed2['price']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.556</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.555</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   506.1</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:21:36</td>     <th>  Log-Likelihood:    </th> <td> -15579.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 15410</td>      <th>  AIC:               </th> <td>3.124e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 15371</td>      <th>  BIC:               </th> <td>3.153e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    38</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -1.2730</td> <td>    0.378</td> <td>   -3.364</td> <td> 0.001</td> <td>   -2.015</td> <td>   -0.531</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>   -0.1583</td> <td>    0.061</td> <td>   -2.593</td> <td> 0.010</td> <td>   -0.278</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_3</th>           <td>   -0.4530</td> <td>    0.061</td> <td>   -7.369</td> <td> 0.000</td> <td>   -0.573</td> <td>   -0.332</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_4</th>           <td>   -0.4925</td> <td>    0.063</td> <td>   -7.806</td> <td> 0.000</td> <td>   -0.616</td> <td>   -0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_5</th>           <td>   -0.5030</td> <td>    0.067</td> <td>   -7.563</td> <td> 0.000</td> <td>   -0.633</td> <td>   -0.373</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_6</th>           <td>   -0.4788</td> <td>    0.083</td> <td>   -5.781</td> <td> 0.000</td> <td>   -0.641</td> <td>   -0.316</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.3149</td> <td>    0.020</td> <td>   15.626</td> <td> 0.000</td> <td>    0.275</td> <td>    0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.1196</td> <td>    0.016</td> <td>   -7.312</td> <td> 0.000</td> <td>   -0.152</td> <td>   -0.088</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_5</th>      <td>    0.1651</td> <td>    0.069</td> <td>    2.382</td> <td> 0.017</td> <td>    0.029</td> <td>    0.301</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_3_0</th>      <td>    0.0201</td> <td>    0.038</td> <td>    0.526</td> <td> 0.599</td> <td>   -0.055</td> <td>    0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_0_75</th>       <td>    0.6805</td> <td>    0.349</td> <td>    1.951</td> <td> 0.051</td> <td>   -0.003</td> <td>    1.364</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.6425</td> <td>    0.334</td> <td>    1.926</td> <td> 0.054</td> <td>   -0.011</td> <td>    1.296</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_25</th>       <td>    0.5229</td> <td>    0.430</td> <td>    1.215</td> <td> 0.224</td> <td>   -0.321</td> <td>    1.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_5</th>        <td>    0.5184</td> <td>    0.334</td> <td>    1.551</td> <td> 0.121</td> <td>   -0.137</td> <td>    1.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_75</th>       <td>    0.5963</td> <td>    0.334</td> <td>    1.785</td> <td> 0.074</td> <td>   -0.058</td> <td>    1.251</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_0</th>        <td>    0.5705</td> <td>    0.334</td> <td>    1.707</td> <td> 0.088</td> <td>   -0.084</td> <td>    1.225</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_25</th>       <td>    0.5571</td> <td>    0.334</td> <td>    1.667</td> <td> 0.096</td> <td>   -0.098</td> <td>    1.212</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>    0.4295</td> <td>    0.334</td> <td>    1.285</td> <td> 0.199</td> <td>   -0.225</td> <td>    1.084</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_75</th>       <td>    0.5568</td> <td>    0.335</td> <td>    1.663</td> <td> 0.096</td> <td>   -0.099</td> <td>    1.213</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_0</th>        <td>    0.5808</td> <td>    0.335</td> <td>    1.732</td> <td> 0.083</td> <td>   -0.077</td> <td>    1.238</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_25</th>       <td>    0.6530</td> <td>    0.336</td> <td>    1.943</td> <td> 0.052</td> <td>   -0.006</td> <td>    1.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_5</th>        <td>    0.6911</td> <td>    0.336</td> <td>    2.059</td> <td> 0.040</td> <td>    0.033</td> <td>    1.349</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_75</th>       <td>    0.7673</td> <td>    0.342</td> <td>    2.245</td> <td> 0.025</td> <td>    0.097</td> <td>    1.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_0</th>        <td>    0.7200</td> <td>    0.346</td> <td>    2.083</td> <td> 0.037</td> <td>    0.043</td> <td>    1.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_25</th>       <td>    0.7534</td> <td>    0.355</td> <td>    2.119</td> <td> 0.034</td> <td>    0.057</td> <td>    1.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_5</th>        <td>    0.7229</td> <td>    0.349</td> <td>    2.074</td> <td> 0.038</td> <td>    0.040</td> <td>    1.406</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_4_75</th>       <td>    0.5629</td> <td>    0.511</td> <td>    1.101</td> <td> 0.271</td> <td>   -0.440</td> <td>    1.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.1053</td> <td>    0.174</td> <td>   -0.606</td> <td> 0.545</td> <td>   -0.446</td> <td>    0.236</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_6</th>         <td>    0.2043</td> <td>    0.170</td> <td>    1.205</td> <td> 0.228</td> <td>   -0.128</td> <td>    0.537</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>    0.5920</td> <td>    0.170</td> <td>    3.483</td> <td> 0.000</td> <td>    0.259</td> <td>    0.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>         <td>    1.0251</td> <td>    0.171</td> <td>    6.003</td> <td> 0.000</td> <td>    0.690</td> <td>    1.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    1.5185</td> <td>    0.172</td> <td>    8.833</td> <td> 0.000</td> <td>    1.182</td> <td>    1.855</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    1.8905</td> <td>    0.174</td> <td>   10.885</td> <td> 0.000</td> <td>    1.550</td> <td>    2.231</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    2.1206</td> <td>    0.180</td> <td>   11.813</td> <td> 0.000</td> <td>    1.769</td> <td>    2.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>     <td>    0.2237</td> <td>    0.061</td> <td>    3.643</td> <td> 0.000</td> <td>    0.103</td> <td>    0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.3877</td> <td>    0.062</td> <td>    6.282</td> <td> 0.000</td> <td>    0.267</td> <td>    0.509</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.6138</td> <td>    0.064</td> <td>    9.602</td> <td> 0.000</td> <td>    0.488</td> <td>    0.739</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4668</td> <td>    0.011</td> <td>   40.670</td> <td> 0.000</td> <td>    0.444</td> <td>    0.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.1189</td> <td>    0.007</td> <td>  -17.865</td> <td> 0.000</td> <td>   -0.132</td> <td>   -0.106</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.958</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.084</td> <th>  Jarque-Bera (JB):  </th> <td>   4.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.033</td> <th>  Prob(JB):          </th> <td>  0.0826</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.058</td> <th>  Cond. No.          </th> <td>    419.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.556\n",
       "Model:                            OLS   Adj. R-squared:                  0.555\n",
       "Method:                 Least Squares   F-statistic:                     506.1\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:21:36   Log-Likelihood:                -15579.\n",
       "No. Observations:               15410   AIC:                         3.124e+04\n",
       "Df Residuals:                   15371   BIC:                         3.153e+04\n",
       "Df Model:                          38                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -1.2730      0.378     -3.364      0.001      -2.015      -0.531\n",
       "bed_2              -0.1583      0.061     -2.593      0.010      -0.278      -0.039\n",
       "bed_3              -0.4530      0.061     -7.369      0.000      -0.573      -0.332\n",
       "bed_4              -0.4925      0.063     -7.806      0.000      -0.616      -0.369\n",
       "bed_5              -0.5030      0.067     -7.563      0.000      -0.633      -0.373\n",
       "bed_6              -0.4788      0.083     -5.781      0.000      -0.641      -0.316\n",
       "floors_1_5          0.3149      0.020     15.626      0.000       0.275       0.354\n",
       "floors_2_0         -0.1196      0.016     -7.312      0.000      -0.152      -0.088\n",
       "floors_2_5          0.1651      0.069      2.382      0.017       0.029       0.301\n",
       "floors_3_0          0.0201      0.038      0.526      0.599      -0.055       0.095\n",
       "bath_0_75           0.6805      0.349      1.951      0.051      -0.003       1.364\n",
       "bath_1_0            0.6425      0.334      1.926      0.054      -0.011       1.296\n",
       "bath_1_25           0.5229      0.430      1.215      0.224      -0.321       1.366\n",
       "bath_1_5            0.5184      0.334      1.551      0.121      -0.137       1.174\n",
       "bath_1_75           0.5963      0.334      1.785      0.074      -0.058       1.251\n",
       "bath_2_0            0.5705      0.334      1.707      0.088      -0.084       1.225\n",
       "bath_2_25           0.5571      0.334      1.667      0.096      -0.098       1.212\n",
       "bath_2_5            0.4295      0.334      1.285      0.199      -0.225       1.084\n",
       "bath_2_75           0.5568      0.335      1.663      0.096      -0.099       1.213\n",
       "bath_3_0            0.5808      0.335      1.732      0.083      -0.077       1.238\n",
       "bath_3_25           0.6530      0.336      1.943      0.052      -0.006       1.312\n",
       "bath_3_5            0.6911      0.336      2.059      0.040       0.033       1.349\n",
       "bath_3_75           0.7673      0.342      2.245      0.025       0.097       1.437\n",
       "bath_4_0            0.7200      0.346      2.083      0.037       0.043       1.398\n",
       "bath_4_25           0.7534      0.355      2.119      0.034       0.057       1.450\n",
       "bath_4_5            0.7229      0.349      2.074      0.038       0.040       1.406\n",
       "bath_4_75           0.5629      0.511      1.101      0.271      -0.440       1.565\n",
       "grade_5            -0.1053      0.174     -0.606      0.545      -0.446       0.236\n",
       "grade_6             0.2043      0.170      1.205      0.228      -0.128       0.537\n",
       "grade_7             0.5920      0.170      3.483      0.000       0.259       0.925\n",
       "grade_8             1.0251      0.171      6.003      0.000       0.690       1.360\n",
       "grade_9             1.5185      0.172      8.833      0.000       1.182       1.855\n",
       "grade_10            1.8905      0.174     10.885      0.000       1.550       2.231\n",
       "grade_11            2.1206      0.180     11.813      0.000       1.769       2.472\n",
       "condition_3         0.2237      0.061      3.643      0.000       0.103       0.344\n",
       "condition_4         0.3877      0.062      6.282      0.000       0.267       0.509\n",
       "condition_5         0.6138      0.064      9.602      0.000       0.488       0.739\n",
       "sqft_living_log     0.4668      0.011     40.670      0.000       0.444       0.489\n",
       "sqft_lot_log       -0.1189      0.007    -17.865      0.000      -0.132      -0.106\n",
       "==============================================================================\n",
       "Omnibus:                        4.958   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.084   Jarque-Bera (JB):                4.987\n",
       "Skew:                          -0.033   Prob(JB):                       0.0826\n",
       "Kurtosis:                       3.058   Cond. No.                         419.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "X = sm.add_constant(X) # adding a constant\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "preds = linreg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.15833939, -0.45296829, -0.49247532, -0.50302899, -0.47881277,\n",
       "        0.31488654, -0.11957762,  0.16509453,  0.02010192,  0.68049705,\n",
       "        0.64253285,  0.52292011,  0.51836034,  0.59626079,  0.57052302,\n",
       "        0.55707267,  0.42947403,  0.55678868,  0.58077201,  0.65303005,\n",
       "        0.69111766,  0.76728591,  0.72004005,  0.75338156,  0.72290234,\n",
       "        0.56293052, -0.10531422,  0.20429802,  0.59201336,  1.02511922,\n",
       "        1.5184747 ,  1.89047858,  2.12060815,  0.22371077,  0.38766671,\n",
       "        0.61378378,  0.46682764, -0.11892803])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.272967211886982"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linreg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5558109155360456"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_train, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  sqft_living_log                with p-value 0.0\n",
      "Add  grade_7                        with p-value 1.32918e-102\n",
      "Add  grade_6                        with p-value 9.59323e-124\n",
      "Add  grade_8                        with p-value 2.47283e-117\n",
      "Add  grade_5                        with p-value 1.00309e-135\n",
      "Add  floors_1_5                     with p-value 8.26546e-92\n",
      "Add  bed_2                          with p-value 3.23429e-70\n",
      "Add  condition_5                    with p-value 2.12013e-65\n",
      "Add  bath_2_5                       with p-value 4.54155e-57\n",
      "Add  sqft_lot_log                   with p-value 2.45185e-58\n",
      "Add  grade_9                        with p-value 7.65984e-46\n",
      "Add  condition_4                    with p-value 2.74956e-40\n",
      "Add  floors_2_0                     with p-value 1.03909e-16\n",
      "Add  grade_11                       with p-value 5.97907e-08\n",
      "Add  grade_10                       with p-value 7.0359e-27\n",
      "Drop grade_6                        with p-value 0.859529\n",
      "Add  bath_1_0                       with p-value 1.37898e-05\n",
      "Add  condition_3                    with p-value 0.000362726\n",
      "Add  bath_3_5                       with p-value 0.0040132\n",
      "Add  bath_1_5                       with p-value 0.00576569\n",
      "Add  bed_4                          with p-value 0.00871912\n",
      "resulting features:\n",
      "['sqft_living_log', 'grade_7', 'grade_8', 'grade_5', 'floors_1_5', 'bed_2', 'condition_5', 'bath_2_5', 'sqft_lot_log', 'grade_9', 'condition_4', 'floors_2_0', 'grade_11', 'grade_10', 'bath_1_0', 'condition_3', 'bath_3_5', 'bath_1_5', 'bed_4']\n"
     ]
    }
   ],
   "source": [
    "result = stepwise_selection(X_train, y_train, verbose = True)\n",
    "print('resulting features:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>price</td>      <th>  R-squared:         </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.553</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1003.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 02 Mar 2022</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:23:09</td>     <th>  Log-Likelihood:    </th> <td> -15624.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 15410</td>      <th>  AIC:               </th> <td>3.129e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 15390</td>      <th>  BIC:               </th> <td>3.144e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>   -0.9238</td> <td>    0.064</td> <td>  -14.394</td> <td> 0.000</td> <td>   -1.050</td> <td>   -0.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living_log</th> <td>    0.4522</td> <td>    0.010</td> <td>   44.789</td> <td> 0.000</td> <td>    0.432</td> <td>    0.472</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_7</th>         <td>    0.3772</td> <td>    0.021</td> <td>   17.613</td> <td> 0.000</td> <td>    0.335</td> <td>    0.419</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_8</th>         <td>    0.8185</td> <td>    0.025</td> <td>   32.602</td> <td> 0.000</td> <td>    0.769</td> <td>    0.868</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_5</th>         <td>   -0.2655</td> <td>    0.053</td> <td>   -5.013</td> <td> 0.000</td> <td>   -0.369</td> <td>   -0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_1_5</th>      <td>    0.3076</td> <td>    0.020</td> <td>   15.360</td> <td> 0.000</td> <td>    0.268</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_2</th>           <td>    0.2697</td> <td>    0.019</td> <td>   14.425</td> <td> 0.000</td> <td>    0.233</td> <td>    0.306</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_5</th>     <td>    0.6037</td> <td>    0.064</td> <td>    9.452</td> <td> 0.000</td> <td>    0.479</td> <td>    0.729</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_2_5</th>        <td>   -0.1540</td> <td>    0.015</td> <td>  -10.487</td> <td> 0.000</td> <td>   -0.183</td> <td>   -0.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot_log</th>    <td>   -0.1226</td> <td>    0.006</td> <td>  -19.928</td> <td> 0.000</td> <td>   -0.135</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_9</th>         <td>    1.3252</td> <td>    0.031</td> <td>   43.384</td> <td> 0.000</td> <td>    1.265</td> <td>    1.385</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_4</th>     <td>    0.3770</td> <td>    0.062</td> <td>    6.113</td> <td> 0.000</td> <td>    0.256</td> <td>    0.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>floors_2_0</th>      <td>   -0.1248</td> <td>    0.015</td> <td>   -8.472</td> <td> 0.000</td> <td>   -0.154</td> <td>   -0.096</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_11</th>        <td>    1.9826</td> <td>    0.058</td> <td>   34.078</td> <td> 0.000</td> <td>    1.869</td> <td>    2.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>grade_10</th>        <td>    1.7244</td> <td>    0.038</td> <td>   45.200</td> <td> 0.000</td> <td>    1.650</td> <td>    1.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_0</th>        <td>    0.0645</td> <td>    0.020</td> <td>    3.307</td> <td> 0.001</td> <td>    0.026</td> <td>    0.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>condition_3</th>     <td>    0.2166</td> <td>    0.061</td> <td>    3.530</td> <td> 0.000</td> <td>    0.096</td> <td>    0.337</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_3_5</th>        <td>    0.0944</td> <td>    0.034</td> <td>    2.807</td> <td> 0.005</td> <td>    0.028</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bath_1_5</th>        <td>   -0.0634</td> <td>    0.023</td> <td>   -2.788</td> <td> 0.005</td> <td>   -0.108</td> <td>   -0.019</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bed_4</th>           <td>   -0.0337</td> <td>    0.013</td> <td>   -2.623</td> <td> 0.009</td> <td>   -0.059</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 2.837</td> <th>  Durbin-Watson:     </th> <td>   2.005</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.242</td> <th>  Jarque-Bera (JB):  </th> <td>   2.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.026</td> <th>  Prob(JB):          </th> <td>   0.245</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.041</td> <th>  Cond. No.          </th> <td>    34.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                  price   R-squared:                       0.553\n",
       "Model:                            OLS   Adj. R-squared:                  0.553\n",
       "Method:                 Least Squares   F-statistic:                     1003.\n",
       "Date:                Wed, 02 Mar 2022   Prob (F-statistic):               0.00\n",
       "Time:                        17:23:09   Log-Likelihood:                -15624.\n",
       "No. Observations:               15410   AIC:                         3.129e+04\n",
       "Df Residuals:                   15390   BIC:                         3.144e+04\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const              -0.9238      0.064    -14.394      0.000      -1.050      -0.798\n",
       "sqft_living_log     0.4522      0.010     44.789      0.000       0.432       0.472\n",
       "grade_7             0.3772      0.021     17.613      0.000       0.335       0.419\n",
       "grade_8             0.8185      0.025     32.602      0.000       0.769       0.868\n",
       "grade_5            -0.2655      0.053     -5.013      0.000      -0.369      -0.162\n",
       "floors_1_5          0.3076      0.020     15.360      0.000       0.268       0.347\n",
       "bed_2               0.2697      0.019     14.425      0.000       0.233       0.306\n",
       "condition_5         0.6037      0.064      9.452      0.000       0.479       0.729\n",
       "bath_2_5           -0.1540      0.015    -10.487      0.000      -0.183      -0.125\n",
       "sqft_lot_log       -0.1226      0.006    -19.928      0.000      -0.135      -0.111\n",
       "grade_9             1.3252      0.031     43.384      0.000       1.265       1.385\n",
       "condition_4         0.3770      0.062      6.113      0.000       0.256       0.498\n",
       "floors_2_0         -0.1248      0.015     -8.472      0.000      -0.154      -0.096\n",
       "grade_11            1.9826      0.058     34.078      0.000       1.869       2.097\n",
       "grade_10            1.7244      0.038     45.200      0.000       1.650       1.799\n",
       "bath_1_0            0.0645      0.020      3.307      0.001       0.026       0.103\n",
       "condition_3         0.2166      0.061      3.530      0.000       0.096       0.337\n",
       "bath_3_5            0.0944      0.034      2.807      0.005       0.028       0.160\n",
       "bath_1_5           -0.0634      0.023     -2.788      0.005      -0.108      -0.019\n",
       "bed_4              -0.0337      0.013     -2.623      0.009      -0.059      -0.009\n",
       "==============================================================================\n",
       "Omnibus:                        2.837   Durbin-Watson:                   2.005\n",
       "Prob(Omnibus):                  0.242   Jarque-Bera (JB):                2.817\n",
       "Skew:                          -0.026   Prob(JB):                        0.245\n",
       "Kurtosis:                       3.041   Cond. No.                         34.2\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fin = X_train[result]\n",
    "X_with_intercept = sm.add_constant(X_fin)\n",
    "model = sm.OLS(y_train,X_with_intercept).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sm.graphics.qqplot(model.resid, dist=stats.norm, line='45', fit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "\n",
    "linreg.fit(X_train, y_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4516765524808653"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "test_residuals = y_hat_test - y_test\n",
    "\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mse = make_scorer(mean_squared_error)\n",
    "\n",
    "cv_5_results = cross_val_score(linreg, X, y, cv=5, scoring=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.445287962313191"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_5_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better MSE, although the R2 is lower, the QQ plot and Residual\n",
    "scatter plots show the data is normalized and homoscedastic, giving us\n",
    "a more accurate view of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranked Feature Selection with Scikit #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "linreg = LinearRegression()\n",
    "selector = RFE(linreg, n_features_to_select = 20)\n",
    "selector = selector.fit(X_fin, y_train.values.ravel()) # convert y to31d np array to prevent DataConversionWarning\n",
    "selector.support_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = X_fin.columns[selector.support_ ]\n",
    "linreg.fit(X_fin[selected_columns],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = linreg.predict(X_fin[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS_Residual = np.sum((y-yhat)**2)\n",
    "SS_Total = np.sum((y-np.mean(y))**2)\n",
    "r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X_fin[selected_columns].shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5531951239929946"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5526435130349613"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x167104e8b50>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAySElEQVR4nO2df4wc53nfv88uh+QelXDJiKmlFU9kUoOqGYY882DRJVBAjCEqlaWcJUuMKgUBEkAt0BSlql5zigWTdGWIBeFIBRIgFeCgCCyoJ4vyRTKdUjLIwAhtyj76jqKvIusfMkmtFIguuYx1t+Tt7b39Y+9dzs6+78w7M+/szuw+H0CAeLs7++7dznfeeZ7v8zwkhADDMAyTXXLdXgDDMAwTDxZyhmGYjMNCzjAMk3FYyBmGYTIOCznDMEzGWdaNN7355pvFhg0buvHWDMMwmeXUqVO/EEKs8/68K0K+YcMGTE5OduOtGYZhMgsRnVf9PHZohYhWEtH3ieg0Ec0Q0YG4x2QYhmHMsbEjvw5glxDiIyJyAPwDEf2dEOKkhWMzDMMwAcQWctEoDf1o6Z/O0n9cLsowDNMhrLhWiChPRNMAPgTwphDiLRvHZRiGYYKxIuRCiLoQYhuA2wB8ioh+y/scInqciCaJaPLSpUs23pZhGIaBZdeKEKJCRH8P4B4AP/I89gKAFwBgeHiYQy8pYmKqjENHz+H9ShW3FgsY3b0JI0Olbi+LYRhDbLhW1hFRcen/CwA+A+Bs3OMynWFiqoynXj2DcqUKAaBcqeKpV89gYqrc7aUxDGOIjdDKLQCOE9HbAH6ARoz8mxaOy3SAQ0fPoVqrt/ysWqvj0NFzXVoRwzBhseFaeRvAkIW1MF3g/Uo11M8Zhkkf3Gulz7m1WAj1c4Zh0gcLeZ8zunsTCk6+5WcFJ4/R3Zu6tCKGYcLSlV4rTHqQ7hR2rTBMdmEhZzAyVGLhZpgMw6EVhmGYjMNCzjAMk3FYyBmGYTIOCznDMEzGYSFnGIbJOOxaYZgluHkYk1VYyJlA+kHgZPMw2XdGNg8D0HOflek9OLTC+NIv3RG5eRiTZVjIGV8OvD7TFwLHzcOYLMNCzmiZmCrjylxN+VivCRw3D2OyDAs5o8Vv191rAsfNw5gsw8lORovfrrvXBI6bhzFZhoWc0XJrsYCyQsyLBQcjQyUjN0vSjhebx/eKubwjYTFn0g6HVhgtunDD/vs3G7lZkna82D5+vzh0mN6DhZzRMjJUwrMPbEGpWAABKBULePaBLRgZKhnZ9ZK29Nk+PlsQmazCoRXGF12vcl383B2KSdrSZ/v4bEFksgrvyHuEiakydh48ho1jR7Dz4LHEwwE61wotrcXvObYcL7aPb+t4nf5bMAwLeQ9gM7ZrKkKjuzeBFD8XuGFbTNrSF+X4fp/Pxno5zs50AxJCdPxNh4eHxeTkZMffN2uYOjJ2HjymdJeUigWcGNsV6v3c/UaAhpDJuLiXDWNHlMchAO8evNf4M8RxnoR5rerzOTnCTSuXoTJXw63FAu66Yx2On70U2QVj62/BMCqI6JQQYtj7c46Rp5QwTZxsxXb9kn0qMStp7InuUETQPFBbzaoEgH+8eg17x6dx6Og5pQCrPl9tUTSrV8uVKg6fKmsvXCZwnJ3pBhxaSSlhHBS2YrthRUgViiA0BNEdtlCFM+TP9o5PR3aKuMMYAFBfurvUhTNMxNT93lFi3Vzqz3QDFvKUEkZUbcWiw4qQ254INERcBuqkmD49caYtZjz6ymmMfv20cjcvMRFd1cVOoroYmIrp+5Uqnp44gyfGp0PHurnUn+kGLOQpJYyo+vm9wxBXhLzZlmqtjpfeutgezqgL1Bb9czM5osCdcJDYex/XJWi9rC44ePHkBeXnCbpTsPW3YJgwcLIzpYRNPLpfF6dk3e/17sdWFxzU6ouYnVfviG2i+9y6xKJElWDUJWjd75UjaD+XO5Fri34Y3MHYIbFkJxGtB/A3AD4GYBHAC0KI/x73uP1OlCZONhKHuuSk99iVqrq9rZc8UTN2HRVVwnViqozZ6wva1+juJHQJWqAh0p8cXI0TP72sPa7tWDdPJmJsYMO1sgDgSSHED4noVwCcIqI3hRD/x8Kx+5ogx4eXsK6TMPjFo3UQGglId+wcAJw8AQKB4RU37jCJ6m4FAHIELIqGWOsueqO7N2H0ldOo1dvfWwA4+bMrvp8naqxbtesGgCdfPt12oVP9zZ6eOIOX3rqIuhDIE+GRO9fjmZEtkdbC9B6xhVwI8QGAD5b+/5dE9A6AEgAW8g6TpPUt7DHc4i1c/5YiO3n+Mr528oLx8YoDTvP/dReVW1YHe7VHhkrY/9qM9o7C7+7h0R2DkS6Iql336CunAaF/P+n8Uf2u6kI0/81izgCWfeREtAHAEIC3FI89DuBxABgcHLT5tj1D3OIZXdvZ1QWn7Wdh0R3biyyw8U4WEmiEWWR7WL+wiIqPri1gYqqMkaFS5AuW/N35hYV0oSACMHz72lBrlij964o7Ai8yzHJtQX0n9NJbF9uEnOPt/Yk1ISeimwAcBrBXCPFP3seFEC8AeAFoJDttvW+vYBIrVT1n7/g09r82g/33b26EDb5+ui1kMTt/QwSjMrp7kzKc0QZBOx7O7fMOS21RNB0jOY3Y+sWvdeEYNwTgkTvX4/CpctvzBBA5dh3njshvve7fwcRUGQden2n53bu/QwAPzehlrNgPichBQ8RfFEK8auOY/UbUtrBAI/EoT9ibVrZfm2t1YVRg44fXM66jVm/EcJNACpNKxINskvtfax8i7YbQCJ08M7IFzz6wRfkZTOyHqiKipIqB5BrlRUp1Aa3W6jjw+gz3f+lxbLhWCMBXAbwjhPjz+EvqT0zCBX47OykylZjDkr235qreI96dn5e6EHDyZBQ+UKELb+SJlGKcJ9LaMuVO1S+c4k2OjgyVsHd8Wvlcv7sJ1R3TE+PTLTkCSZSEr5dH7lwPIDgRrRN4G0lwJh3YCK3sBPAHAM4Q0fTSz/5MCPEtC8fOFHHik7oYtHs3FxSnlu8bdBy/9XuFyJ1kk0m6IIEuFpzQMXA3O35jDX544Wqbh14nVotCGFkmVeiaWfldTOSxvX9rlaD6JXyB1nCH39/2sR2DTddKjoAVy3J48eQFHD97KVKoCmhNqLKgZxsbrpV/AIwK5nqauH5gVQzaHS6YmCpjbt5fHKWg6I4TVOyjssJ5Mdll/9O1GmJsNPHz/1fFsw9sabszUFVbAvqLlIllUheO0f0e6kJg24E3MDu/0PxdyL910HtJEXdfONzfDb/Oic+MbMEzI1uU3zPvbt9NwcljxbKc9o6Efeu9AXc/tERcD7d8jtsat9JppDBMdpZSrHWFRAC0Fxr5WNzCHUkcEQcadxZeD/3Og8eUYuX1drsvVkHLWDPgaP82foVDKlGs1upNH7sffrvgoIs5oP6eqUI3QOPOaP/9mwHA9/vDYZbsw0JuCVse7usLi83/vzLXSGKudHLa2PCiEEZhnKBkathinyQZWJ7HzoPH8H6liuKAAyH0laQCeldPEEJA6+Yxdum4WBQwyg3Ii+jk+ctt+YdnH9jScjHPUePi/sT4tG/4Re72/cJ6h46e076e2+xmm74S8iQ9tnFi0xKd2PrFhr19P3QhHt0xwp7ANpJ0QczO1zE731iXX1IVaFzMNo4dwa3FAubmF0IJr9vt4/0euO9swsSgVy1fhlUrlgWGPKq1elv+4alXz+DB7aWWi3mj58uNv6UOv8EV7u+9LvbPbXazTd90P0x6BJeN9qVhRVUAbd0BdRcDnSXw1mJBWzBEaCTZZCe/YsHBTSuWobaYnMUwLHUhmn9PE9H34mcpHBkq4cTYrkDLpZur1RpOjO3Czw/ei+f2bAv1Wl23yCD8Wgd4v/c66+Zdd6zjOaMZpm+EPMyghih425euGXCwYlkOT4xPxx5KUCw4bRcJifeCpLsY1IXQXmh0mlwccPDMyBacGNuF5/Zsw/WFxaZYyh4qWaHg5H3L4f1QXaR1eKcjmbbOlUTJU7jDS150Cd88UbPN7oPbSzh8qsw+8wzTN0LeiRFc8sQtDji4MldDpVozOjFkEYm8HXdDAD679RbfYhz3BUl3MZB9sYuu3bdMpuq85+6f+1nrsoDfXQkBbX8bd2HPoaPn8OD2UkuP8cd2DBrdgR06ei7x35Pfrl93kZJhuRNju3D87KVENzlM8vRNjNxGDDsIv2SbzhngfY33pBcADp8qY/j2tTgxtgsbx44ohUFekFTd/Zw84a471rUVxshkqrzweHE3quqFZJiqEyMgy+/fbtovc0tPkpHqcqWKF09eaFZ+SoZvXxuYc0n69+YXvpuYKmvj9CZ/26j+dKbz9I2Qm1i74hLkW1adMCZeZ/dFwOiC5Dlza3Wh7TRYrdWxYllO6ba4MlfDtgNv4LNbb9H2N8ka+uTjjQSjKo8rALx48gKGb1/bUgEalCw3bTYWBlO3kt/dwNVqrenYWV1wtK6goS+9gcpcjfuzpJy+EfIogxrCErT7Uu3+TXds7h233wXp0NFzoR0lV6s17clcqdZCtZsNi5+zIwylYgGz1xeMB15EQQDNcIPJ90g3/MLEb+5HXQj8fMmtJENAqrX4fbcWxY3PMutTaCbv1LhwKN30jZAD4Qc1hMVv96Xb/Zvu2ORFIOiCFOVW/tYl/7ENCk4eK51coINEYkPEn9+zDSNDJTw9cSbRiw7QbueU/VT2jk83rX1FnzF4q5bnMb+wiMUYdze5pVB/UDWx304bQLOlsGlPnGqtjidfPt30tPMOPT30lZAnja6IRFbYeUeVSY9y0K7UexHwuyCFvZWXxw7rlwba+5G4e4jomk4lwYHXZzB5/jLGf3Ax8fdSNe6SvwH5u/ATz2u1xdghqkUBbP7i/1ZeKNxJSr+dNtCIk4f9m7tbEXOL3PTAw5ctYzocwiv47oZKqo6DpidGmOpG9wUmbFWk7uKzZsDBvvs2489efRtzrrgz0zkIZhf0AScX+29ULDi4vrAYekg4Ew3d8GUW8phEqRb1a46kqs7TzXv0a4DlN87M/X7ui0ZxwMH1Wj3w5LYV104TcePWaUKW6nfz4/hVmjLRYSFPANUuVu5GAL3Q6iyEBASW3AONcWqg1k6Eql2QaUfDMBQD4q5B9OJFIG08tmMwVntbW8g7Aw612IOFPAF0O+scNWKpbveInGVZmatprXyqXYzuPVTI17t38MUBBx9dW7DWG0UWn3RbJBg9+Rxhx8Y1OPHTy91eCgAOtdhEJ+Sc7IyBzumxKNDmSqgtipbydi86V0sYN0m5Um1Lgl2Zq8HJE4ga3f7iwgLeHVSxaB31RYHvWhTxONOeAG6T2wlYyGMQt9gjqLBjYqocuhBH5WSIcxIy6UD2FTd1F9n4i8s5pgCa04mi8n6lmmj30X6HhTwEqnmWqonrpqja0LrfSzfsQRUjZ3oX9wCMkaFSqHBbVLxDKeLmWQRaLalybCDABUY26JumWXFRtcE9fKqMB7eXIrd09evz4le6v+dT63Ho81tDtUgtODnjDn5MurgyV8PGsSPYsNRi9q471iX+nrInukkLiajU6gIHXp9J5Nj9Bgu5Ibo2uMfPXsJXHt4aWiSD+rz4xcYPn2p06gvTK3u+LnDbmpWh1sikB7kflhuIgpPsqSurOJPe+ZtWADP+sJAb4tcG19uL3ISgLL7fbr1aq2PvUp9z091ZfVHgxx/OGq6OSTPVWh0rnXxjWlOCdKpJGvc9jw8LuQJ3L2o5FEInrO4eKKO7Nxm3xQ2KC5oMM5C7s4Q3Z0wKuTJXS70h3zTkyEMs4tPzEqAS5aDnq0bC3XXHOt9BAu7XBaEaZOBF7vKDqNbq4Er4/iTJuak2UE2lUsFDLOLT00IeZU6nXyzcHT6RE3fc3QiNp7cDePLl04EXF87mM1lGniMmeZxeGFzSTXrafug3p1MnkkGx8LCv06HqIufulRI09Zxh0gyh8d3e/9oMavXgW0YZkmSveTR6WsijzOmMOhIuTnGQ++Li7a0SJOLcu4RJI/I7adqXp1ypYuhLb7S0k+BhFub0dGglKEGpQpVkNBkJF2bSuoqyq/ItTCvZf/7rqzI1zZ5hdFyZq7XF/au1Ova/xl7zIHpayKOIstdK6I2FB73OPaVe4uQJxYIDgn8m3zRZKhEAfvzhLO/ImZ6mUm3MjmVnix4r3Q+J6K8BfBbAh0KI3wp6fie7H3Yj5qYq5Xf3/PbrRsgxcYZRw10UE25jS0T/CsBHAP4mbULebZT9xAO6yRWcfGJl0QyTZfp9YIVOyK2EVoQQ3wGQjubHXcDPq66KedfqQhtikaGcqP1bGKaXYZuiGmuDJYhoA4Bv6nbkRPQ4gMcBYHBwcPv58+etvG+n0IVoguZv+sW8vTtv962jbie/avmyWBN6mP5h1fK8sq1xlikWHKxasaxv7YmJTwgKEnI3WQut+I10izJ9XiJHcum+lPLiUXb5yeWczfHvX0x9ZR/DJE3ByeOTg6tx8mdXUBeNO91H7lyPZ0aCq6KzCAt5DPyGJccZcqtK3jw9cSawib+TIxZxJrN0Ige0ankeX/5c7yVGE42R9zp+hUWmTbJUeHtMPD1xBl87eSHQtcIizmSVMGX7cZidr/dVMy4rQk5ELwH4HoBNRPQeEf2xjeOmBb/CIpNCIL+0pfsi8dJbF6Msj2EyAQFNx8ns9YXE30/2VDdtmJdlbLlWHhFC3CKEcIQQtwkhvmrjuGnBr7DIXUAEtIt2wcnj0R2DWheK+yLB/nGml1ldcLDtwBvYOz7dsYR9XYhmw7wnxqfx9MSZjrxvp+npXiu2cHc4VCUm3c20dO6W4dvXKhOmssq0V79gDCOZnV/o6pxZAeDFkxcwfPva3oud20p2hiFryU5buEW+OODgWq2OKjcTZ/qANQNOasa6ZbmoSJfs5B25RYLaAcid+8RUGaNfPx0racml/EyWSIuIA71ZVMRCbgmv11zXgnNiqownXz5tLMJEwMpl7XatFcsItUV09VaVYbJIHKdZWmH7oSX8hlhIpNiH2Uk/eucgnn1gS9vU9LnaIuos4gwTCgICW1JnERZyS5gMsQjba/yxHYN4ZmQLJs9fVsbSObrOMOEQ6M0hFRxasYTJZKEwsblbiwU8M7IFE1NlfO3kBStrZJh+R9qEe22kHAt5TNz9ULxj17xDLMKMg5Oif+B1no7CMDZw8oTZ6wvYMHak5VwtV6rYOz6NL3zjDObm65kUdg6txEDGvKU4C9woCFJNFlIVFumqPuVOPk3ZfobJMrW6aBYiqbJLs/P1ZvFQ1sr7eUceA1XMW7avVflUVYVFd92xDodPlbWFQgzDdB5Z3g9kI6bOQh4DkwSnF3cVqBvZ8TBPhAe3q5/DMEznqAuhtBCnERbyGJgkOIOSKhNTZRw+VW5aEutCYPz7F3Hk7Q84rMIwXaZaq2P/azOpF3KOkcfAr5kW0BpD18XeDrw+0z4KblEEiviq5f4dFxmGsUOlWkt9vJyFPAbuzoeE9gRnUJHQxFQ58q77y5/rzQkoDJNG3IV9aYRDKzHRxbyB4Bh61C/HmgEHI0Ml7B2fjvR6hmHCkfb+LLwjTxC/gRRA9C9HZa6GjWNHIq+LYZhwpL0/Cwt5ggTF0KN+OQTUPliGSTOEzuV21gw41o6VBTswh1ZCMjFVxv7XZpqFBWsGHOy7b7MyvBI0kGJ096a2YRMM06sIAE4+ByD577stx1cpI1WeLOQhUPURvzJXw+gr+sIBvxi6W+hVJf4M00ukabiECe5xjmmHQyshOHT0nHIYRK0uIicuR4ZKODG2Cz8/eC+e27Mt8eniDNMtsiTiQHsb6jTDO/IQ+CUnTROXugIh988ZhkkHpk3uug0LeQj8uheaJC51U4Qmz19u67fCMEz3yZOurV264NBKCEZ3b4KTa//DOnkyymrrCoReeusiizjDpJCszMXlHXkIZNJD51oJ6quiC5tk5cvCMP2GTRtjkrCQh0TnQjEZvqwLzeSJWMwZxjIf//VV+PGHs7GOEee07OQUIhZyS/j1VfHzjRecPB7cXuIYOcNYZm4+/lRbeecdhFe0vXMGVBs7m7CQL2HSbjZK2MT986ACIZ7NyTD2sOEAIzTOfT/xVd2Nv3jyQltNiHdjZxMWcgSHReKETbxuFl1o5sjbH9j7QAzDIGchZCmAQPHVTQpTkZS92IprhYjuIaJzRPQTIhqzccxOEtRuNuhxQN1XRQ573Th2BDsPHvPtaZy1YgmGSTu28k5B4htGnJNqvhVbyIkoD+AvAfwugE8AeISIPhH3uJ0kKCxiGjZx9yZfM+AAohFjCxro+vTEmdifgWGYZAgSX93jXqNyks23SMS8ahHRpwHsF0LsXvr3UwAghHhW95rh4WExOTkZ6f32/I/vRXqdH6fOX8GCovR+WY6w/fY1mLpQwXy9PXGyPJ/D0GBRecwwr3nr3cuR1s0wTLLkCNh48yrcfNMK7XN+8dF1vPuLWbglhAAQofmzZTnC7b82gJtvWoHxf/vpyOsholNCiOG2dUY+4g1KAC66/v3e0s+8C3iciCaJaPLSpUsW3tYeukuZ/Pn6tQV464By1Pi5DpWIq37+i4+uG66SYZikIaDlXCeDys6bb1qBjTevwvJ8Q06XLR3ALeyLCduLbSQ7VZ+0bdVCiBcAvAA0duRR3yzO1UyHbkjD4qJovp90rZQr1abv+1ptEY98alCZCNl58Jgy+VksOHjvSrXpWpmbX7D7YRiGCUWeCItCKG2D9UWB9yvX8B92fdzYbaI69xcFcK22mIh+AXZ25O8BWO/6920A3rdw3I6hi3HliJox7ZGhUjOhKZMofnFvVfITaMTM3cOYOcnJMN3lKw9vxXN7tgFoWICDjA1B6HJq5UrVyPgQBRtC/gMAHyeijUS0HMDvA3jNwnE7hk5060K0CLWJe0UyMlTCg9tLytsVhmHSw+T5y3jq1TO+nQ7Llaqx+PolR4OMD1GJLeRCiAUAfwLgKIB3ALwshJiJe9xOIh0nqk5nbqE2ca+4OX72Eg+KYJiUo9qFqxh95bSR+Oo2hm5s9zq3UhAkhPgWgG/ZOFa3GBkq4QnNVHop1KZFP97XMQyTfWp1gQOvzwT2T/FWcHeiOIjb2LoImnofNEzZ9HgMw2STK3OtOS5diERO/nr34L3aqV829SEzQj4xVcbOg8cSSxYAwULtLfopFQt49oEt2my2yS0WwzDZxSREoppj4OTMZhiYkoleKya9TmwQ1NRKPsf0Pb3DlRmG6T2MQiTe9JtlF0Tsys4ohK3s1HmyS8UCToztsrm0RNF9jjUDDj66tqAc7MwwTPdZtTwPJ59TtrUN0iGb+pVkZWfihHWLpBVVqIUA3Pvbt+DQQ1u7syiG6VPyRCA0ivRWLdeHQJ084cuf24L9928OlSOTdEK/MiHkQUnIrDAyVMInB1e3/EwAGP/+RfULGIZJjF8tLMNze7Zhet/dmPnSPXh+zzalBblWF81WtmFyZJJO6FcmQiveGDnQuBKa/BLTxMRUGXs1FsdiwcEvry3wyDeG6TCExoaqpLEXy+e8e/DeSMe3qV+ZDq1EvRKmDb/sdqVawyN3rtc+zjBMMsitU7lS1eYg4+yeO6FfmXCtAOHcImklKCZ2/OwlKwNjGYaJhsCNHbrERh/xpPUrEzvyXqE44Pg+Xq5U8d6Va9j5m2u5RwvDdAkZZsnS3X9mduRpx2R480fXglvWVmt1nPzZFe7RwjBdImu2ZoCF3AomBUuHjp4z9olzwpNhukOS49iShIXcAn7tbaWQh/GM5jXTv4sFB7PXuXCIYZIgT9QSRpmYKuPA6zPNmQHFgoP9929OZZiFY+QWMDH8hxnQ+sid65WFB5/deov10l6G6TdyBOX59ZWHt7aI+Ogrp1sGv1SqNYx+3ayVrYok+0WxkFvAxPCva8j16I7BtsTKMyNblHal42cvoVbn3TjDxOHf3DkYaAc8dPSc8lyrLYpIfcRl+NWkc2IUOLRigdHdm5SGf3eszaQhlxu3Xck9L5RhmGgQgEd3DOKZkS0A/Bvu+YVCo5TWm4Rf48BCbgFTkY7iJVVVhTFMWshSRXKYykzdEBn5WFiS7rfCQm6JpAz/qis5w6SFWn0xEyKuG+6gY3T3Joy+crotvBK1j3jY6WJh4Rh5lzBNfHA4hUkzs/N14/y7kyPkc53P1kexFI4MlXDo81uxxlXEVyw4OPTQ1kgbtrDTxcLCO/KQBBX+mLxudcHB7PxC82rvNyhDZ0VkmLRg8u3ME+FTG9fguz+9nPh6JAS0naNhzl+bd9lhc2RhYSEPQdRJRd7XqZrT6xIfLOJML1AXAic6KOKq6sxOTRrTkWS/FQ6thMAv8xz2dSrciQ8ZemGYfiROAIYAZcgi6vmbBVjIQxA182yamZaJD7fnlGH6DXd9hR9rBhzlxK1Hdwwqd7668ylrk8ZUsJCHIOqkD5PMtDvxwU4Vpl9xF8WdGNulFXMCsO++zS2FPcWCg+KAgxdPXmgzEExMlRPpNZ4WWMhDEDXzrHqdkyMMODd+/TkC9r82g41jR3gnzvQ1T4xPN4VYdy7IzFGzUI4auacrczVl5eSho+eUSVldGCZrsJCHIOqkD9Xr9nxqPYRrjzA7X0elWuP2tUxf4y1h17kVCWgJP6o8Ae74ty58ItCZRGfS9IRrJaolMAq6zHPQGrwl90++fDqUI8XJEUDgXitM6nFyhJtWLmtpOBUFv/CiCHhcIgVcV5ATtlAorcTakRPRQ0Q0Q0SLRNQ2ELQTJN2MxvYa5HPDiHiOGs16WMSZtJMnwqGHtmLqi3enQiRl/DvpgpxuEze08iMADwD4joW1RCINlqIwawiTyCwWHDh5ArcfZ7KAtxXs6O5NcPLxKjmLhXZnSsHJt1Rc+q1HCnWvDHDXESu0IoR4BwCIutckO+lmNLbXYLqugpMHcSiFyQgE4MHtjfChO8wYxxBecPLYf/9mAO0VkQB8m8mtGXCw777N2vBmr5H5GHnSzWhsr0H3XCJg9UoHV6u15pf1ifFp4zWUigW8X6miOOCgMsdJUyZZVi3PY3b+hogKAIdPNUKJh0+VbwhsxC9iyZNnUgnw5PnLePHkhZa38Laq7RcCQytE9G0i+pHiv98L80ZE9DgRTRLR5KVLl6Kv2EMaYl9h1qB77nMPb8P0vrvx7sF7cWJsF0aGSsYXozw1OrK9e/BeTH3x7ugfhGEMmZtv3wlXa3W89NbF2DUQj+0YxOjuTTh09JxvU7njZy+1XSfE0s/7jcAduRDiMzbeSAjxAoAXAGB4eNjahjHpZjRx1gAAOw8eU67LZL26Vppe6kK09Izw66XMMDbQfSNt9AZ69dR7Lbt6XU8U05BmJ11t3YKEhV88Ef09gP8shJg0ef7w8LCYnDR6amZRDYQoOHnfBIvqCwegZQCsH7JR0MRUGU+MT3N4hek4SXbr9DbC2nnwmNZSKJ8X5TxMM0R0SgjR5hCMaz/8HBG9B+DTAI4Q0dE4x+slwrppdBZGAJj64t14fs+2tpCMF7kTGRkqsYgzieHnGPmNdQOJzQf37rRNQpppcLV1glhCLoT4hhDiNiHECiHEPxNC7La1sKwT1k0T9IVz26d0FF0nWBo8vEzvsWbAwb2/fYv28R9/OBt7E6EzwXlzRiaWwjS42jpB5l0raSWsm8bkCye/oHs1bpYrczXsPHgMd92xDpW5+ZArZphgrszVEk0mEoBH7xxsdb5Abx4IshSmwdXWCbjXSkKEddOYdlYMuiUsV6r42skLLdYwhrFJkol0AWD49rV4cHsJ+aWteZ6o6VEPSxpcbZ2Ad+QRMemtApi7aUZ3b1ImZbxfOHajML3OgddncK12Y6hzXQgcPlXG8O1rjcXcO1pxpZNDZa7Ws64VFvIImI6MClNJZiL8newfwzBJ4uduUTm0qrU6nnz5NIAb54puM6UarVhw8nhuz7aeE3CJFfthWNJoPwzjNTWxPdnm6YkzbVVsDJNVnt+zLZJFVloHgfYSfflYs0e5hyTPz06hsx/yjhzhh7J2OhM+MVVmEWd6hjUDDkaGSvj65IXQA5ndTi6dy6tfnCpuONmJ8F7TqCPfoqKbbsIw3aLg5FEsBHcg9OLkCfvu24yJqTJ+eOFqy2Om/vP3K1WtKJeX7qhVCEBb7p91WMgRfofd6Uy4zZ2Eqi0owwDmtQdFmTys1trE10+MS8UCDn2+0eZWtXky3azcWixoxZoA3HXHOu13vBvzCjoBCznC77A73dvYb6dfLDh4bMeg0XFkW9BnH9jStHYxDNBIPr5fqQZ+L5wccH1hsZmQFLgh3qViAf/yN9cqxd3bCCuq+0pumEZ3b1JeNASAb57+wLd4rhcrOzlGDnPrn5tO9jYe3b1JWwR0tVrDN09/EHiMYsHB/vtb+zOPfv00ajy1ggFarH5+1BaB2mL7Tlq2nX3q1TPKjoTj37+I8R9cDGwAVyw4+OX1BdQV30tva1vdOVGpNi4yJ8Z2YePYEeVOv9fi5bwjR/qnh4wMlbBqufpWcXXBaX5xVcjd0PS+u9uskTet1F/Hc/DvqcEwbt6vVH2nX5mMKpR3jF95aGvLufj8nm34uau9s8Tv7kHuuDudz+oWvCNfohM77CCLo58vdn5hUXnM2fkF3/f0a7Jf8emo+Ocuz+22A2/4XiyYbEC4EQqxfR9269Jgk6jkCC3Vmybnot/dg1xLlLvtLMI78g4RNKDZ7/FDR89pQyC1uvBNMB0+VdYmdnS7klKx0HIi7b9/c2Id7Rg7rBlQJ7HlpjVP1AyBPLpjsLnjtZErkcIYZ5e7KPy/qyr87hjlWtJ+t20LFvIOEWRx9Hs8aKfjt7vSJXYmpsqYvd6+m1ftVrgtbvqpzNXaBOv5Pdvw3MON9sdy91quVHH4VLk5UeorD29tuwA4OQo1NFkKo8rNFeaYYZKQE1NlfHRNfTfq5KnlOzwyVMKJsV0t07d6DQ6tdIggi6Pf43En/qgmpqgG1+aovXWuhAjoQhEwY0huaWftrVzcefCYdoPgDieqBprsf20mMKTmvntzH6u85ICpC9FMUgLAky+fNgqJBKG7SyVC0+LYT7CQd4igdpp+j8tBzKqvPyE44SkLIWTMXZeUkueFqrLVRMRlr+peq0JNIqYclZ2/uRY/vHC17e/nHfcnQ3K6DYC3PbJK+LzH8f4edHdvqrzPE+PTuLVYwCN3rm9rUevGNDyjFXxhFl/vNTi00iGCioj8HvcLbQhAWZjhxR1zN9n1RPHaVuZqeGZkCx7dMdhTMfUgES84uVChiKjkc4SHhge1dQDyb+bOt+gwFUwZlvj5wXvx3J5toWLNqrzP4VNlPLi9pKwKDZOE7Bc3iiks5B0iKOkS9HhQ1Z27MEOXwKrW6tj/2kzzNjwIKfimCagcESamysrp5r1KqVjAO//1d3Ho81sTt2vWF0UzJKILT5QrVewdn/adZB/VtRE21qzL+xx5+wNM72uML4yahOyXPuOmcGilgwRZHP0eV9movMhT+2OrV2p3Y2FshHJ0nOnOXN7e+63RjZOnQG9x2pHCIf92E1Nlo9hyVOTFNeqQY29RTZLo7vyuzNUwMVU2tvz62XZNO5b2OizkGcH7xfU7hW0Nn5A6EcYfXK3VtSJTLDhYtWJZy4nnF8dNO6uW51viyPJz2eh+oIvL31osYGKqHFnEg9q4hmnnHIRfkl7eWQQR1Jm0X4XbCwt5StGdUPKLq+uJbpOrS7vKsK6ZuhAoOPm2nTnRjR3sgddntCXWbnJ0IwnbTfI5aikbd/KEL39ui1Jo4uB2eKgKWe66Y11TyMJgEnYI2845CL/WEmHcKX6uG6YBx8hTSFDxEKCOEQahK/PXIRNHunikLiYs453ehNaVuRpGXzmN//TytHIKjIpFgdiJRF3OwPSoxYLTVjbu18UvCk6O8Pyebc3Ysy5ncvzspdDvZxp/DtvOOYiRIXVSE4jvTum1Xilx4R15ypiYKiu9tt5diNeza4KTz6HgtDfk1+GO/8r38nqNdeXPUuS8seIoMfFVy5c1QzJRNueqOwRZoDJXU7c+kMj+H7rbeFuCUnMlMiXu9wyyE9ogCdHcf//mWCXyQbZdpgHvyFOE3Inr4p/eE0q6CJ7fs81od3612l79p9sxFQtOm6jIMmzZIAmAr9PGlshVqjX849VrkZ0wcl1yncWCAxDaRHzNQKMlsPt3stLxP0V0glIsOM33M0X3+zKxE/qhuqObmCpj58Fj2Dh2pDlsIQlLX9wSeXanmMEzO1NEUNw7T4RFIbQNtw68PuMbslAlu56eOIOvnbzQ9tzHdgxi+Pa1LZPIZ+cXWnbUckai6qTU3Vl0GtUa/Wau6pos+X1O7/NlolIezyQXIN9flYy0lQ+Rx1etueDk8eD2UluxTtBn74RrpFPvkwV4ZmcGCNrBuvtleJNQbvvbf3nlNOYVIYwNv9a+szp+9pLyvY68/UHLSa2y0+mSTkF3Fp1EtaP2GxNmEtZy4w1xud0m8u804OSMQjh33bEOOw8eaxMskzubNQMOBpb7h5/kcXSx8ONnLzWHFweJpu3EqB/sTgmGQyspIswtrC4JNTJUwsBy9fX5uz+93Fbc4+f1NYmlq15vKwFYCAhrmHBlrtYWVvD7PZuGtdzIEFepWGgT0WqtjuXL8nByrUGWHDXCLzLcIHfDqgR30Pei4OSx777NzWIdXfGYPI5fLNy06Md2YpSJBwt5igjrRFHdbk9MlbXFKALtxT1xk0aq14eNjXvjyAUnvzS+zk7Zu1dgojh+TH5Pus99tVrDIZfrpVhwsLrg4Gq11tz1qtwoct2q9brHq3lDH0FxZRuxcHaTpItYQk5Eh4joLBG9TUTfIKKipXX1JTIxZIrKVhe0I/KeaDqRMNkN65JOYS8OMp4c12bnh7dJlN9MRy+myTU/gZQ73ef2bGvOvHTvvP2aW6kShs9ppuZ4P58qwWgjgci9TtJF3Bj5mwCeEkIsENF/A/AUgD+Nv6z+Rdr2TJJbqjBA0I7Ie6KNDJUwef5yS8dCAWBhUcDJUUurUCdHuGnlMlTmar7xU79ujSpUSb4nDBOEpqg+98hQCUNfekOZIM4tte0Nk1xTJUoJDbGW3Sd1IQm/Sk73ek3xe76N8vZ+mbyTFWIJuRDiDdc/TwL4fLzlMIBZXxVA3UjLrwpTd6KpmlzV6qIlgRbmZB8ZKhk7Nfx29UEXM0IjmVkNSCTmXBWlXnT52F9d6WB6392+x/VikvjU/U0F0HbhjCKMpg6PuAlE7nWSLmy6Vv4IwLjuQSJ6HMDjADA4OGjxbXsP70lSHHDw0bUFo5NcdxEoFpxmYYsX3S6+MlfD1BfDiZmkpBFi04uDaZOwlU4eAPk+z6/E/6omn1Cp1pQOkiCkQKosg359aADgppXLIl04JZ10kshjsnCng0AhJ6JvA/iY4qEvCCH+duk5XwCwAOBF3XGEEC8AeAFo+MgjrbaP0DXoN9lpAWjxlPuJOJBM9Zzu1nvfffp1uDFtElaZq+G5PdsCw1E6+6DfUA55vCiCqLs4+lky41w4Ae5L0s8ECrkQ4jN+jxPRHwL4LIDfEd2oLuoTwu5+rrnCDZVqzVeIkoh32rj1NmkSJhOJul2wRCespp0Kwwqi7uJYKhYwN7+gjMvHTRSyk6R/ietauQeN5Ob9Qog5O0ti4hLW4xu3jFqH26kBNBKYshw8LCZOi9Hdm7SGRZ1IVgybdwHhBNFvvfvu25xI2Tk7SfqXuDHyvwCwAsCb1NjanBRC/LvYq2JiEWVnllS801bc1mSHr3LgAP4iGaZFbxhBNFmv7UQhO0n6F+610oP49RIJGizQC2sJ05tD1XfEyRMg0JZctnGXkjTcl6S34V4rfUQSO7OoAtGNuG2Yuwu/Fr1ZFER2kvQnLOQ9iG2Pb5zwSBb6SevEjwWRyQos5D2KzZ1ZHFsbx23tw+ETxgsLORNInPBIEhWA/SxknS76YbIBCzkTSNzwiM27g34XMi76YVRwG1smkDSN2+r3Pthc9MOoYCFnAkmqYCgK/S5kXPTDqODQCmNEWmxt3XTBpCE2z8ljRgULOZMpuiVkNmLz3gvBXXesw/Gzl0JdGLh9LKOCKzuZzCEFsVypNtvClhIWtLgVqqoKUi9ZqR5luoeuspNj5Iw1JqbK2HnwGDaOHYncHMuEkaFSMwEr28K6hxUnQdzYvMlA6n5K2jJ2YSFnrCB3nKop8EnQafdK3CSjqeD3S9KWsQsLOWOFTgtrp90rcS2YpoLP7hMmCizkjBU6LaydtuHFtWCqLgRe2H3CRIVdK4wVOm0L7IZ7JY4FU+U2ieJaYRgVLOSMFTotrFm04aXFi8/0HizkjBW6IawsjAzTgIWcsQYLK8N0B052MgzDZBwWcoZhmIzDQs4wDJNxWMgZhmEyDgs5wzBMxulK90MiugTgfMffODluBvCLbi+iA/TD5+TP2Bv06me8XQixzvvDrgh5r0FEk6rWkr1GP3xO/oy9QT98RjccWmEYhsk4LOQMwzAZh4XcDi90ewEdoh8+J3/G3qAfPmMTjpEzDMNkHN6RMwzDZBwWcoZhmIzDQm4JIjpERGeJ6G0i+gYRFbu9JtsQ0UNENENEi0TUU9YuIrqHiM4R0U+IaKzb60kCIvprIvqQiH7U7bUkBRGtJ6LjRPTO0nf1P3Z7TZ2AhdwebwL4LSHEbwP4vwCe6vJ6kuBHAB4A8J1uL8QmRJQH8JcAfhfAJwA8QkSf6O6qEuF/Arin24tImAUATwoh/gWAHQD+fY/+LVtgIbeEEOINIcTC0j9PAritm+tJAiHEO0KIZKYpd5dPAfiJEOJnQoh5AP8LwO91eU3WEUJ8B8Dlbq8jSYQQHwghfrj0/78E8A6Anm+Sz0KeDH8E4O+6vQjGmBKAi65/v4c+OPl7HSLaAGAIwFtdXkri8ISgEBDRtwF8TPHQF4QQf7v0nC+gcXv3YifXZguTz9iDkOJn7MvNMER0E4DDAPYKIf6p2+tJGhbyEAghPuP3OBH9IYDPAvgdkVGDftBn7FHeA7De9e/bALzfpbUwMSEiBw0Rf1EI8Wq319MJOLRiCSK6B8CfArhfCDHX7fUwofgBgI8T0UYiWg7g9wG81uU1MREgIgLwVQDvCCH+vNvr6RQs5Pb4CwC/AuBNIpomor/q9oJsQ0SfI6L3AHwawBEiOtrtNdlgKUn9JwCOopEce1kIMdPdVdmHiF4C8D0Am4joPSL6426vKQF2AvgDALuWzsNpIvrX3V5U0nCJPsMwTMbhHTnDMEzGYSFnGIbJOCzkDMMwGYeFnGEYJuOwkDMMw2QcFnKGYZiMw0LOMAyTcf4/+t4NrWGLOS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "residuals = y - preds\n",
    "\n",
    "plt.scatter(preds, residuals)\n",
    "plt.hlines(0, preds.min(), preds.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
